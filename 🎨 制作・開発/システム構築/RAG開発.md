# RAGé–‹ç™ºå…¥é–€ã‚¬ã‚¤ãƒ‰

**æ‰€è¦æ™‚é–“**: 2-3æ™‚é–“  
**ãƒ¬ãƒ™ãƒ«**: åˆç´šã€œä¸­ç´š  
**å‰æçŸ¥è­˜**: åŸºæœ¬çš„ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°çŸ¥è­˜ï¼ˆPythonæ¨å¥¨ï¼‰

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- RAGï¼ˆRetrieval Augmented Generationï¼‰ã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£ã™ã‚‹
- å®Ÿéš›ã«RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹
- ä¸»è¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆLangChainã€LlamaIndexï¼‰ã®ä½¿ã„æ–¹ã‚’å­¦ã¶
- å®Ÿç”¨çš„ãªRAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹

## ğŸ“‹ ç›®æ¬¡

1. [RAGã¨ã¯ä½•ã‹ï¼Ÿ](#ragã¨ã¯ä½•ã‹)
2. [RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆè¦ç´ ](#ragã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆè¦ç´ )
3. [é–‹ç™ºç’°å¢ƒã®æº–å‚™](#é–‹ç™ºç’°å¢ƒã®æº–å‚™)
4. [ã¯ã˜ã‚ã¦ã®RAGå®Ÿè£…](#ã¯ã˜ã‚ã¦ã®ragå®Ÿè£…)
5. [LangChainã§RAGæ§‹ç¯‰](#langchainã§ragæ§‹ç¯‰)
6. [LlamaIndexã§RAGæ§‹ç¯‰](#llamaindexã§ragæ§‹ç¯‰)
7. [RAGã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„](#ragã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„)
8. [å®Ÿè·µçš„ãªå¿œç”¨ä¾‹](#å®Ÿè·µçš„ãªå¿œç”¨ä¾‹)
9. [ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•](#ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•)

---

## RAGã¨ã¯ä½•ã‹ï¼Ÿ

**RAGï¼ˆRetrieval Augmented Generationï¼‰**ã¯ã€æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ãŸAIæŠ€è¡“ã§ã™ã€‚

### å¾“æ¥ã®LLMã¨RAGã®é•ã„

| å¾“æ¥ã®LLM | RAG |
|-----------|-----|
| è¨“ç·´æ™‚ã®çŸ¥è­˜ã®ã¿ | å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚‚æ´»ç”¨ |
| çŸ¥è­˜ã®æ›´æ–°ãŒå›°é›£ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±å–å¾— |
| ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºç”Ÿ | æ ¹æ‹ ã®ã‚ã‚‹å›ç­” |

### RAGãŒè§£æ±ºã™ã‚‹å•é¡Œ

1. **çŸ¥è­˜ã®é®®åº¦** - æœ€æ–°æƒ…å ±ã¸ã®å¯¾å¿œ
2. **å°‚é–€çŸ¥è­˜** - ç¤¾å†…æ–‡æ›¸ã‚„ç‰¹å®šåˆ†é‡ã®çŸ¥è­˜
3. **å›ç­”ã®æ ¹æ‹ ** - æƒ…å ±æºã®æ˜ç¤º

## RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆè¦ç´ 

### 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
- **æ–‡æ›¸åé›†**: PDFã€Wordã€Webãƒšãƒ¼ã‚¸ãªã©
- **å‰å‡¦ç†**: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- **ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²

### 2. ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- **åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«**: ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
- **ãƒ™ã‚¯ãƒˆãƒ«DB**: åŠ¹ç‡çš„ãªé¡ä¼¼æ¤œç´¢

### 3. æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- **é¡ä¼¼åº¦æ¤œç´¢**: ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’å–å¾—
- **ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°**: æ¤œç´¢çµæœã®ç²¾åº¦å‘ä¸Š

### 4. ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ**: æ¤œç´¢çµæœã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- **LLMç”Ÿæˆ**: æœ€çµ‚çš„ãªå›ç­”ç”Ÿæˆ

## é–‹ç™ºç’°å¢ƒã®æº–å‚™

### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install langchain
pip install langchain-community
pip install langchain-openai
pip install chromadb
pip install pypdf
pip install python-dotenv
```

### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
OPENAI_API_KEY=your_openai_api_key_here
```

## ã¯ã˜ã‚ã¦ã®RAGå®Ÿè£…

### ã‚·ãƒ³ãƒ—ãƒ«ãªRAGã‚·ã‚¹ãƒ†ãƒ 

```python
import os
from dotenv import load_dotenv
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
documents = [
    "RAGã¯æ¤œç´¢æ‹¡å¼µç”Ÿæˆã®ç•¥ã§ã™ã€‚",
    "RAGã‚·ã‚¹ãƒ†ãƒ ã¯å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ã—ã€ãã®æƒ…å ±ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
    "ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯æ–‡æ›¸ã®æ„å‘³çš„æ¤œç´¢ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚"
]

# 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.create_documents(documents)

# 3. åŸ‹ã‚è¾¼ã¿ã®ä½œæˆ
embeddings = OpenAIEmbeddings()

# 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
vectorstore = Chroma.from_documents(texts, embeddings)

# 5. æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
llm = OpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 6. è³ªå•ã¨å›ç­”
question = "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
answer = qa_chain.run(question)
print(f"è³ªå•: {question}")
print(f"å›ç­”: {answer}")
```

## LangChainã§RAGæ§‹ç¯‰

### PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ãŸRAG

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

class PDFRAGSystem:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path
        self.setup_rag()
    
    def setup_rag(self):
        # 1. PDFèª­ã¿è¾¼ã¿
        loader = PyPDFLoader(self.pdf_path)
        documents = loader.load()
        
        # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
        embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma.from_documents(texts, embeddings)
        
        # 4. ä¼šè©±ãƒã‚§ãƒ¼ãƒ³ä½œæˆ
        llm = ChatOpenAI(temperature=0)
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=self.vectorstore.as_retriever(),
            memory=memory
        )
    
    def ask(self, question):
        response = self.qa_chain({"question": question})
        return response["answer"]

# ä½¿ç”¨ä¾‹
rag_system = PDFRAGSystem("document.pdf")
answer = rag_system.ask("ã“ã®æ–‡æ›¸ã®ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ")
print(answer)
```

## LlamaIndexã§RAGæ§‹ç¯‰

### ã‚·ãƒ³ãƒ—ãƒ«ãªLlamaIndexå®Ÿè£…

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding

class LlamaIndexRAG:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.setup_index()
    
    def setup_index(self):
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        documents = SimpleDirectoryReader(self.data_dir).load_data()
        
        # 2. ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
        llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
        embed_model = OpenAIEmbedding()
        
        service_context = ServiceContext.from_defaults(
            llm=llm,
            embed_model=embed_model
        )
        
        # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        self.index = VectorStoreIndex.from_documents(
            documents,
            service_context=service_context
        )
        
        # 4. ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
        self.query_engine = self.index.as_query_engine()
    
    def query(self, question):
        response = self.query_engine.query(question)
        return response.response

# ä½¿ç”¨ä¾‹
rag = LlamaIndexRAG("./documents")
answer = rag.query("ã“ã®æ–‡æ›¸ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„")
print(answer)
```

## RAGã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„

### 1. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æœ€é©åŒ–

```python
# ç•°ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
chunk_sizes = [200, 500, 1000, 1500]
best_score = 0
best_size = 0

for size in chunk_sizes:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=size//5
    )
    # è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
    score = evaluate_rag_system(text_splitter)
    if score > best_score:
        best_score = score
        best_size = size

print(f"æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {best_size}")
```

### 2. æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè£…
def create_hybrid_retriever(documents):
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents, embeddings)
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = 5
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢
    ensemble_retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[0.7, 0.3]
    )
    
    return ensemble_retriever
```

### 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹å–„

```python
from langchain.prompts import PromptTemplate

# ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
custom_prompt = PromptTemplate(
    template="""ä»¥ä¸‹ã®æ–‡è„ˆã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ç­”ãˆãŒã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
å¿…ãšæ–‡è„ˆã«åŸºã¥ã„ã¦å›ç­”ã—ã€æƒ…å ±æºã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

æ–‡è„ˆ:
{context}

è³ªå•: {question}

å›ç­”:""",
    input_variables=["context", "question"]
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ããƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": custom_prompt}
)
```

## å®Ÿè·µçš„ãªå¿œç”¨ä¾‹

### 1. ç¤¾å†…FAQã‚·ã‚¹ãƒ†ãƒ 

```python
class CompanyFAQSystem:
    def __init__(self, faq_documents):
        self.setup_rag(faq_documents)
    
    def setup_rag(self, documents):
        # RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        texts = text_splitter.create_documents(documents)
        
        embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma.from_documents(texts, embeddings)
        
        llm = ChatOpenAI(temperature=0)
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=self.vectorstore.as_retriever()
        )
    
    def get_answer(self, question):
        # é–¢é€£åº¦ã®ä½ã„è³ªå•ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if self.is_relevant_question(question):
            return self.qa_chain.run(question)
        else:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãã®è³ªå•ã«ã¤ã„ã¦ã¯å›ç­”ã§ãã¾ã›ã‚“ã€‚"
    
    def is_relevant_question(self, question):
        # é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯ã®ãƒ­ã‚¸ãƒƒã‚¯
        relevant_docs = self.vectorstore.similarity_search(question, k=1)
        return len(relevant_docs) > 0 and relevant_docs[0].metadata.get('score', 0) > 0.7
```

### 2. æ–‡æ›¸è¦ç´„ã‚·ã‚¹ãƒ†ãƒ 

```python
from langchain.chains.summarize import load_summarize_chain

class DocumentSummarizer:
    def __init__(self):
        self.llm = ChatOpenAI(temperature=0)
        self.summarize_chain = load_summarize_chain(
            self.llm,
            chain_type="map_reduce"
        )
    
    def summarize_with_rag(self, documents, query=None):
        if query:
            # RAGã‚’ä½¿ã£ã¦ç‰¹å®šã®è¦³ç‚¹ã‹ã‚‰è¦ç´„
            retriever = self.create_retriever(documents)
            relevant_docs = retriever.get_relevant_documents(query)
            return self.summarize_chain.run(relevant_docs)
        else:
            # å…¨ä½“ã®è¦ç´„
            return self.summarize_chain.run(documents)
```

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

### 1. æ¤œç´¢ç²¾åº¦ãŒä½ã„

**å•é¡Œ**: é–¢é€£æ€§ã®ä½ã„æ–‡æ›¸ãŒæ¤œç´¢ã•ã‚Œã‚‹

**è§£æ±ºç­–**:
- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®èª¿æ•´
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å°å…¥

```python
# æ”¹å–„ä¾‹
def improve_retrieval_accuracy():
    # ã‚ˆã‚Šè‰¯ã„åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    retriever = vectorstore.as_retriever(
        search_kwargs={
            "k": 10,
            "filter": {"category": "æŠ€è¡“æ–‡æ›¸"}
        }
    )
    
    return retriever
```

### 2. å›ç­”ãŒé•·ã™ãã‚‹

**å•é¡Œ**: å†—é•·ãªå›ç­”ãŒç”Ÿæˆã•ã‚Œã‚‹

**è§£æ±ºç­–**:
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å›ç­”ã®é•·ã•ã‚’æŒ‡å®š
- temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´

```python
concise_prompt = PromptTemplate(
    template="""ä»¥ä¸‹ã®æ–‡è„ˆã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å¯¾ã—ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ï¼ˆ100æ–‡å­—ä»¥å†…ï¼‰ã€‚

æ–‡è„ˆ: {context}
è³ªå•: {question}
ç°¡æ½”ãªå›ç­”:""",
    input_variables=["context", "question"]
)
```

### 3. å‡¦ç†é€Ÿåº¦ãŒé…ã„

**å•é¡Œ**: å¤§é‡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æ¤œç´¢ãŒé…ã„

**è§£æ±ºç­–**:
- ãƒ™ã‚¯ãƒˆãƒ«DBã®æœ€é©åŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨
- ä¸¦åˆ—å‡¦ç†ã®å°å…¥

```python
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…ä¾‹
from functools import lru_cache

class CachedRAGSystem:
    @lru_cache(maxsize=100)
    def cached_search(self, query):
        return self.vectorstore.similarity_search(query)
```

### 4. ã‚³ã‚¹ãƒˆç®¡ç†

**å•é¡Œ**: APIä½¿ç”¨æ–™ãŒé«˜é¡ã«ãªã‚‹

**è§£æ±ºç­–**:
- ãƒ­ãƒ¼ã‚«ãƒ«LLMã®æ´»ç”¨
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹APIå‘¼ã³å‡ºã—å‰Šæ¸›
- ãƒãƒƒãƒå‡¦ç†ã®å°å…¥

```python
# ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ä¾‹
def cost_effective_rag():
    # ã‚ˆã‚Šå®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    
    # æ¤œç´¢çµæœã‚’åˆ¶é™
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever
    )
```

## ğŸ“ ã¾ã¨ã‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€RAGã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤ã‹ã‚‰å®Ÿè·µçš„ãªå¿œç”¨ã¾ã§å­¦ç¿’ã—ã¾ã—ãŸï¼š

1. **åŸºæœ¬æ¦‚å¿µ** - RAGã®ä»•çµ„ã¿ã¨åˆ©ç‚¹
2. **å®Ÿè£…æ–¹æ³•** - LangChainã¨LlamaIndexã§ã®æ§‹ç¯‰
3. **æ”¹å–„æŠ€è¡“** - ç²¾åº¦å‘ä¸Šã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
4. **å®Ÿç”¨ä¾‹** - å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹å ´é¢ã§ã®æ´»ç”¨
5. **å•é¡Œè§£æ±º** - ã‚ˆãã‚ã‚‹èª²é¡Œã¸ã®å¯¾å‡¦æ³•

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã™** - ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’è©¦ã—ã¦ã¿ã‚‹
2. **è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§å®Ÿé¨“** - èº«è¿‘ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§RAGã‚’æ§‹ç¯‰
3. **è©•ä¾¡æŒ‡æ¨™ã®å­¦ç¿’** - RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½è©•ä¾¡æ–¹æ³•
4. **æœ¬ç•ªé‹ç”¨ã®æº–å‚™** - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

RAGæŠ€è¡“ã¯æ€¥é€Ÿã«é€²æ­©ã—ã¦ã„ã‚‹ãŸã‚ã€ç¶™ç¶šçš„ãªå­¦ç¿’ã¨å®Ÿé¨“ãŒé‡è¦ã§ã™ã€‚