# ChatGPTã§ã®Pythonå®Ÿè¡Œæ´»ç”¨

**æ‰€è¦æ™‚é–“**: 1æ™‚é–“  
**ãƒ¬ãƒ™ãƒ«**: ä¸­ç´š  
**å‰æçŸ¥è­˜**: [åŸºç¤è¬›åº§ç¾¤](../basic/)å®Œäº†ã€Pythonã®åŸºæœ¬æ–‡æ³•

## å­¦ç¿’ç›®æ¨™

ã“ã®è¬›åº§ã‚’ä¿®äº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š
- ChatGPTã®Code Interpreterï¼ˆAdvanced Data Analysisï¼‰ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã§ãã‚‹
- ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–ã‚’ChatGPTä¸Šã§å®Ÿè¡Œã§ãã‚‹
- ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ»å¤‰æ›ä½œæ¥­ã‚’è‡ªå‹•åŒ–ã§ãã‚‹
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°æŠ€è¡“ã‚’ç¿’å¾—ã§ãã‚‹

## ğŸ“‹ ç›®æ¬¡

1. [ChatGPT Code Interpreterã¨ã¯](#chatgpt-code-interpreterã¨ã¯)
2. [åŸºæœ¬çš„ãªPythonå®Ÿè¡Œ](#åŸºæœ¬çš„ãªpythonå®Ÿè¡Œ)
3. [ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–](#ãƒ‡ãƒ¼ã‚¿åˆ†æå¯è¦–åŒ–)
4. [ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ»å¤‰æ›](#ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤‰æ›)
5. [é«˜åº¦ãªæ´»ç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯](#é«˜åº¦ãªæ´»ç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯)

---

## ChatGPT Code Interpreterã¨ã¯

### ğŸ¤– Code Interpreterã®ç‰¹å¾´

**ChatGPT Code Interpreter**ï¼ˆAdvanced Data Analysisï¼‰ã¯ã€ChatGPTå†…ã§Pythonã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã§ãã‚‹å¼·åŠ›ãªæ©Ÿèƒ½ã§ã™ã€‚

#### ä¸»ãªç‰¹å¾´
```
âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ Pythonå®Ÿè¡Œ
âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
âœ… ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ï¼ˆmatplotlibã€plotlyç­‰ï¼‰
âœ… è±Šå¯Œãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½
âœ… ã‚¨ãƒ©ãƒ¼è‡ªå‹•ä¿®æ­£ãƒ»ãƒ‡ãƒãƒƒã‚°æ”¯æ´
âœ… ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªåˆ†æç’°å¢ƒ
```

#### åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆä¸»è¦ãªã‚‚ã®ï¼‰
```python
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»åˆ†æ
import pandas as pd
import numpy as np
import scipy
import statsmodels

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px

# æ©Ÿæ¢°å­¦ç¿’
import scikit-learn
from sklearn import datasets, model_selection, metrics

# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
import json
import csv
import xml.etree.ElementTree as ET
import openpyxl
import PIL (Pillow)

# ãã®ä»–
import requests
import re
import datetime
import os
import zipfile
```

### ğŸ”„ Code Interpreter vs é€šå¸¸ã®Pythonç’°å¢ƒ

| ç‰¹å¾´ | Code Interpreter | ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ | Google Colab |
|------|------------------|-------------|---------------|
| **è¨­å®šä¸è¦** | âœ… | âŒ | âœ… |
| **ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†** | âœ… | âœ… | âœ… |
| **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¿½åŠ ** | åˆ¶é™ã‚ã‚Š | âœ… | âœ… |
| **ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–** | ã‚»ãƒƒã‚·ãƒ§ãƒ³é™ã‚Š | âœ… | åˆ¶é™ã‚ã‚Š |
| **AIæ”¯æ´** | âœ… | âŒ | âŒ |
| **ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚¢ã‚¯ã‚»ã‚¹** | âŒ | âœ… | âœ… |

---

## åŸºæœ¬çš„ãªPythonå®Ÿè¡Œ

### ğŸš€ åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨åŸºæœ¬æ“ä½œ

#### Code Interpreterã®æœ‰åŠ¹åŒ–
```
ChatGPT Plus/Team/Enterprise ã§åˆ©ç”¨å¯èƒ½

ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•:
1. ChatGPT ã«ãƒ­ã‚°ã‚¤ãƒ³
2. æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹
3. GPT-4 ã‚’é¸æŠ
4. "Advanced Data Analysis" ã‚’é¸æŠ
   ã¾ãŸã¯
   "Code Interpreter" ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–

ç¢ºèªæ–¹æ³•:
ãƒãƒ£ãƒƒãƒˆç”»é¢ã« ğŸ“ ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚¢ã‚¤ã‚³ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹
```

#### åŸºæœ¬çš„ãªPythonå®Ÿè¡Œ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

print("Hello, ChatGPT Code Interpreter!")
print("Python version:", import sys; sys.version)

# ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç¢ºèª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("Pandas version:", pd.__version__)
print("NumPy version:", np.__version__)

# ç°¡å˜ãªè¨ˆç®—
numbers = [1, 2, 3, 4, 5]
result = sum(numbers)
print(f"Sum of {numbers} = {result}")
ã€
```

### ğŸ“Š åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æ“ä½œ

#### ãƒªã‚¹ãƒˆã¨è¾æ›¸ã®æ“ä½œ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿æ“ä½œã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

# å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
sales_data = [
    {"month": "1æœˆ", "sales": 1200000, "target": 1000000},
    {"month": "2æœˆ", "sales": 1350000, "target": 1100000},
    {"month": "3æœˆ", "sales": 980000, "target": 1200000},
    {"month": "4æœˆ", "sales": 1450000, "target": 1300000},
    {"month": "5æœˆ", "sales": 1600000, "target": 1400000}
]

# åˆ†æå®Ÿè¡Œ
total_sales = sum(item["sales"] for item in sales_data)
total_target = sum(item["target"] for item in sales_data)
achievement_rate = (total_sales / total_target) * 100

print(f"ç·å£²ä¸Š: {total_sales:,}å††")
print(f"ç·ç›®æ¨™: {total_target:,}å††") 
print(f"é”æˆç‡: {achievement_rate:.1f}%")

# æœˆåˆ¥é”æˆç‡ã®è¨ˆç®—
for item in sales_data:
    rate = (item["sales"] / item["target"]) * 100
    status = "é”æˆ" if rate >= 100 else "æœªé”"
    print(f"{item['month']}: {rate:.1f}% ({status})")
ã€
```

#### Pandasã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€ŒPandasã‚’ä½¿ã£ã¦ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
data = {
    'product': ['PC', 'Tablet', 'Phone', 'Watch', 'Earphones'] * 4,
    'region': ['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka'] * 5,
    'sales': np.random.randint(100, 1000, 20),
    'profit': np.random.randint(10, 200, 20),
    'date': pd.date_range('2024-01-01', periods=20, freq='D')
}

df = pd.DataFrame(data)

# åŸºæœ¬çµ±è¨ˆæƒ…å ±
print("=== ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ± ===")
print(df.info())
print("\n=== åŸºæœ¬çµ±è¨ˆ ===")
print(df.describe())

# ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†è¨ˆ
print("\n=== å•†å“åˆ¥å£²ä¸Šåˆè¨ˆ ===")
product_sales = df.groupby('product')['sales'].sum().sort_values(ascending=False)
print(product_sales)

print("\n=== åœ°åŸŸåˆ¥å£²ä¸Šåˆè¨ˆ ===")
region_sales = df.groupby('region')['sales'].sum().sort_values(ascending=False)
print(region_sales)

# åˆ©ç›Šç‡ã®è¨ˆç®—
df['profit_rate'] = (df['profit'] / df['sales']) * 100
print("\n=== åˆ©ç›Šç‡TOP5 ===")
top_profit = df.nlargest(5, 'profit_rate')[['product', 'region', 'sales', 'profit', 'profit_rate']]
print(top_profit)
ã€
```

---

## ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–

### ğŸ“ˆ åŸºæœ¬çš„ãªã‚°ãƒ©ãƒ•ä½œæˆ

#### Matplotlibã‚’ä½¿ã£ãŸåŸºæœ¬ã‚°ãƒ©ãƒ•
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

import matplotlib.pyplot as plt
import numpy as np

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ']
sales = [1200, 1350, 980, 1450, 1600, 1380]
target = [1000, 1100, 1200, 1300, 1400, 1500]

# å›³ã®ã‚µã‚¤ã‚ºè¨­å®š
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 1. æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
ax1.plot(months, sales, marker='o', linewidth=2, label='å®Ÿç¸¾', color='#2E86AB')
ax1.plot(months, target, marker='s', linewidth=2, label='ç›®æ¨™', color='#A23B72', linestyle='--')
ax1.set_title('æœˆåˆ¥å£²ä¸Šæ¨ç§»', fontsize=14, fontweight='bold')
ax1.set_ylabel('å£²ä¸Šï¼ˆä¸‡å††ï¼‰')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. æ£’ã‚°ãƒ©ãƒ•ï¼ˆæ¯”è¼ƒï¼‰
x = np.arange(len(months))
width = 0.35

bars1 = ax2.bar(x - width/2, sales, width, label='å®Ÿç¸¾', color='#2E86AB', alpha=0.8)
bars2 = ax2.bar(x + width/2, target, width, label='ç›®æ¨™', color='#A23B72', alpha=0.8)

ax2.set_title('å®Ÿç¸¾ vs ç›®æ¨™', fontsize=14, fontweight='bold')
ax2.set_ylabel('å£²ä¸Šï¼ˆä¸‡å††ï¼‰')
ax2.set_xticks(x)
ax2.set_xticklabels(months)
ax2.legend()

# æ£’ã‚°ãƒ©ãƒ•ã«æ•°å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ 
for bar in bars1:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
             f'{height}', ha='center', va='bottom')

for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
             f'{height}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# é”æˆç‡ã®è¨ˆç®—ã¨è¡¨ç¤º
achievement_rates = [(s/t)*100 for s, t in zip(sales, target)]
print("=== æœˆåˆ¥é”æˆç‡ ===")
for month, rate in zip(months, achievement_rates):
    status = "âœ… é”æˆ" if rate >= 100 else "âŒ æœªé”"
    print(f"{month}: {rate:.1f}% {status}")
ã€
```

#### Seabornã‚’ä½¿ã£ãŸé«˜åº¦ãªå¯è¦–åŒ–
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€ŒSeabornã‚’ä½¿ã£ã¦ä»¥ä¸‹ã®åˆ†æã¨ã‚°ãƒ©ãƒ•ä½œæˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n_samples = 200

data = pd.DataFrame({
    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], n_samples),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),
    'sales_amount': np.random.gamma(2, 500, n_samples),
    'customer_age': np.random.normal(35, 12, n_samples),
    'satisfaction_score': np.random.beta(2, 1, n_samples) * 10,
    'purchase_frequency': np.random.poisson(3, n_samples) + 1
})

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
data['customer_age'] = data['customer_age'].clip(18, 70)
data['satisfaction_score'] = data['satisfaction_score'].clip(1, 10)

print("=== ãƒ‡ãƒ¼ã‚¿åŸºæœ¬æƒ…å ± ===")
print(data.head())
print(f"\\nãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data.shape}")

# è¤‡æ•°ã‚°ãƒ©ãƒ•ã®ä½œæˆ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Šåˆ†å¸ƒ
sns.boxplot(data=data, x='product_category', y='sales_amount', ax=axes[0,0])
axes[0,0].set_title('å•†å“ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Šåˆ†å¸ƒ', fontweight='bold')
axes[0,0].tick_params(axis='x', rotation=45)

# 2. åœ°åŸŸåˆ¥é¡§å®¢æº€è¶³åº¦
sns.violinplot(data=data, x='region', y='satisfaction_score', ax=axes[0,1])
axes[0,1].set_title('åœ°åŸŸåˆ¥é¡§å®¢æº€è¶³åº¦åˆ†å¸ƒ', fontweight='bold')

# 3. å¹´é½¢ã¨å£²ä¸Šã®é–¢ä¿‚
sns.scatterplot(data=data, x='customer_age', y='sales_amount', 
                hue='product_category', alpha=0.6, ax=axes[1,0])
axes[1,0].set_title('å¹´é½¢ vs å£²ä¸Šé‡‘é¡', fontweight='bold')

# 4. è³¼å…¥é »åº¦ã¨æº€è¶³åº¦ã®é–¢ä¿‚
sns.regplot(data=data, x='purchase_frequency', y='satisfaction_score', ax=axes[1,1])
axes[1,1].set_title('è³¼å…¥é »åº¦ vs æº€è¶³åº¦', fontweight='bold')

plt.tight_layout()
plt.show()

# ç›¸é–¢åˆ†æ
print("\\n=== æ•°å€¤å¤‰æ•°ã®ç›¸é–¢ä¿‚æ•° ===")
numeric_cols = ['sales_amount', 'customer_age', 'satisfaction_score', 'purchase_frequency']
correlation_matrix = data[numeric_cols].corr()
print(correlation_matrix.round(3))

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ç›¸é–¢ã‚’å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5)
plt.title('å¤‰æ•°é–“ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontweight='bold')
plt.show()
ã€
```

### ğŸ“Š Plotlyã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚°ãƒ©ãƒ•

#### å‹•çš„ã‚°ãƒ©ãƒ•ã®ä½œæˆ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€ŒPlotlyã‚’ä½¿ã£ã¦ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
dates = pd.date_range('2024-01-01', '2024-06-30', freq='D')
products = ['Product A', 'Product B', 'Product C', 'Product D']

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
np.random.seed(42)
sales_data = []

for product in products:
    base_trend = np.cumsum(np.random.normal(0, 10, len(dates)))
    seasonal = 50 * np.sin(2 * np.pi * np.arange(len(dates)) / 30)  # æœˆæ¬¡ã‚µã‚¤ã‚¯ãƒ«
    noise = np.random.normal(0, 20, len(dates))
    sales = 1000 + base_trend + seasonal + noise
    
    for date, sale in zip(dates, sales):
        sales_data.append({
            'date': date,
            'product': product,
            'sales': max(0, sale),  # è² ã®å£²ä¸Šã‚’é˜²ã
            'profit_margin': np.random.uniform(0.1, 0.3)
        })

df = pd.DataFrame(sales_data)
df['profit'] = df['sales'] * df['profit_margin']

# 1. æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆ
fig1 = px.line(df, x='date', y='sales', color='product',
               title='å•†å“åˆ¥å£²ä¸Šæ¨ç§»ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰',
               labels={'sales': 'å£²ä¸Šé‡‘é¡', 'date': 'æ—¥ä»˜'})

fig1.update_layout(
    xaxis_title="æ—¥ä»˜",
    yaxis_title="å£²ä¸Šé‡‘é¡",
    legend_title="å•†å“",
    hovermode='x unified'
)

fig1.show()

# 2. è¤‡åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
fig2 = make_subplots(
    rows=2, cols=2,
    subplot_titles=('å•†å“åˆ¥å£²ä¸Šåˆè¨ˆ', 'æ—¥åˆ¥å£²ä¸Šæ¨ç§»', 'åˆ©ç›Šãƒãƒ¼ã‚¸ãƒ³åˆ†å¸ƒ', 'å£²ä¸Š vs åˆ©ç›Š'),
    specs=[[{'type': 'bar'}, {'type': 'scatter'}],
           [{'type': 'histogram'}, {'type': 'scatter'}]]
)

# å•†å“åˆ¥å£²ä¸Šåˆè¨ˆï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
product_totals = df.groupby('product')['sales'].sum().reset_index()
fig2.add_trace(
    go.Bar(x=product_totals['product'], y=product_totals['sales'], 
           name='å£²ä¸Šåˆè¨ˆ', marker_color='lightblue'),
    row=1, col=1
)

# æ—¥åˆ¥åˆè¨ˆå£²ä¸Šæ¨ç§»
daily_sales = df.groupby('date')['sales'].sum().reset_index()
fig2.add_trace(
    go.Scatter(x=daily_sales['date'], y=daily_sales['sales'],
               mode='lines', name='æ—¥åˆ¥å£²ä¸Š', line=dict(color='orange')),
    row=1, col=2
)

# åˆ©ç›Šãƒãƒ¼ã‚¸ãƒ³åˆ†å¸ƒ
fig2.add_trace(
    go.Histogram(x=df['profit_margin'], name='åˆ©ç›Šãƒãƒ¼ã‚¸ãƒ³', 
                 marker_color='lightgreen'),
    row=2, col=1
)

# å£²ä¸Š vs åˆ©ç›Š
fig2.add_trace(
    go.Scatter(x=df['sales'], y=df['profit'], mode='markers',
               name='å£²ä¸Š vs åˆ©ç›Š', marker=dict(color='red', size=4)),
    row=2, col=2
)

fig2.update_layout(height=800, showlegend=True, 
                   title_text="å£²ä¸Šåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
fig2.show()

# 3. 3Dãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ
monthly_data = df.groupby(['product', pd.Grouper(key='date', freq='M')]).agg({
    'sales': 'sum',
    'profit': 'sum',
    'profit_margin': 'mean'
}).reset_index()

fig3 = px.scatter_3d(monthly_data, x='sales', y='profit', z='profit_margin',
                     color='product', size='sales',
                     title='3Dåˆ†æ: å£²ä¸Š x åˆ©ç›Š x åˆ©ç›Šç‡',
                     labels={
                         'sales': 'å£²ä¸Š',
                         'profit': 'åˆ©ç›Š', 
                         'profit_margin': 'åˆ©ç›Šç‡'
                     })

fig3.show()

# ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
print("=== ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ ===")
print(f"æœŸé–“: {df['date'].min()} ï½ {df['date'].max()}")
print(f"ç·å£²ä¸Š: {df['sales'].sum():,.0f}")
print(f"ç·åˆ©ç›Š: {df['profit'].sum():,.0f}")
print(f"å¹³å‡åˆ©ç›Šç‡: {df['profit_margin'].mean():.1%}")

print("\\n=== å•†å“åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ===")
product_summary = df.groupby('product').agg({
    'sales': ['sum', 'mean'],
    'profit': 'sum',
    'profit_margin': 'mean'
}).round(2)
print(product_summary)
ã€
```

---

## ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ»å¤‰æ›

### ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»å‡¦ç†

#### CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œï¼‰
ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

# ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèª
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
df = pd.read_csv('uploaded_file.csv')

print("=== ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ± ===")
print(f"è¡Œæ•°: {len(df)}")
print(f"åˆ—æ•°: {len(df.columns)}")
print(f"åˆ—å: {list(df.columns)}")

print("\\n=== ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ± ===")
print(df.dtypes)

print("\\n=== æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ ===")
missing_data = df.isnull().sum()
if missing_data.sum() > 0:
    print("æ¬ æå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:")
    print(missing_data[missing_data > 0])
else:
    print("æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“")

print("\\n=== ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
print(df.head(10))

print("\\n=== åŸºæœ¬çµ±è¨ˆ ===")
print(df.describe())

# æ•°å€¤åˆ—ã®å¯è¦–åŒ–
numeric_columns = df.select_dtypes(include=[np.number]).columns
if len(numeric_columns) > 0:
    fig, axes = plt.subplots(len(numeric_columns), 1, 
                            figsize=(10, 4*len(numeric_columns)))
    if len(numeric_columns) == 1:
        axes = [axes]
    
    for i, col in enumerate(numeric_columns):
        df[col].hist(bins=30, ax=axes[i], alpha=0.7)
        axes[i].set_title(f'{col} ã®åˆ†å¸ƒ')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('é »åº¦')
    
    plt.tight_layout()
    plt.show()

# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®åˆ†æ
categorical_columns = df.select_dtypes(include=['object']).columns
if len(categorical_columns) > 0:
    print("\\n=== ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å€¤åˆ†å¸ƒ ===")
    for col in categorical_columns[:5]:  # æœ€åˆã®5åˆ—ã®ã¿
        print(f"\\n{col}:")
        print(df[col].value_counts().head(10))
ã€
```

#### Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
import matplotlib.pyplot as plt

# Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰
excel_file = 'uploaded_file.xlsx'

# ã‚·ãƒ¼ãƒˆåã‚’ç¢ºèª
xl = pd.ExcelFile(excel_file)
print("=== ã‚·ãƒ¼ãƒˆä¸€è¦§ ===")
for i, sheet in enumerate(xl.sheet_names):
    print(f"{i+1}. {sheet}")

# å„ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
sheet_data = {}
for sheet_name in xl.sheet_names:
    try:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        sheet_data[sheet_name] = df
        print(f"\\n=== {sheet_name} ã‚·ãƒ¼ãƒˆæƒ…å ± ===")
        print(f"è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
        print(f"åˆ—å: {list(df.columns)}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if len(df) > 0:
            print("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
            print(df.head(3))
            
    except Exception as e:
        print(f"{sheet_name} ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ¡ã‚¤ãƒ³ã‚·ãƒ¼ãƒˆã®åˆ†æï¼ˆæœ€åˆã®ã‚·ãƒ¼ãƒˆã‚’ä»®å®šï¼‰
if sheet_data:
    main_sheet_name = list(sheet_data.keys())[0]
    main_df = sheet_data[main_sheet_name]
    
    print(f"\\n=== {main_sheet_name} è©³ç´°åˆ†æ ===")
    
    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
    numeric_cols = main_df.select_dtypes(include=['number']).columns
    if len(numeric_cols) > 0:
        print("\\næ•°å€¤åˆ—ã®çµ±è¨ˆ:")
        print(main_df[numeric_cols].describe())
        
        # ç›¸é–¢åˆ†æ
        if len(numeric_cols) > 1:
            correlation = main_df[numeric_cols].corr()
            print("\\nç›¸é–¢ä¿‚æ•°:")
            print(correlation.round(3))
    
    # æ–°ã—ã„Excelãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†æçµæœã‚’å‡ºåŠ›
    with pd.ExcelWriter('analysis_result.xlsx', engine='openpyxl') as writer:
        # å…ƒãƒ‡ãƒ¼ã‚¿
        main_df.to_excel(writer, sheet_name='åŸãƒ‡ãƒ¼ã‚¿', index=False)
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        if len(numeric_cols) > 0:
            summary_df = main_df[numeric_cols].describe()
            summary_df.to_excel(writer, sheet_name='çµ±è¨ˆã‚µãƒãƒªãƒ¼')
        
        # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±
        info_df = pd.DataFrame({
            'åˆ—å': main_df.columns,
            'ãƒ‡ãƒ¼ã‚¿å‹': main_df.dtypes.values,
            'æ¬ æå€¤æ•°': [main_df[col].isnull().sum() for col in main_df.columns],
            'ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°': [main_df[col].nunique() for col in main_df.columns]
        })
        info_df.to_excel(writer, sheet_name='ãƒ‡ãƒ¼ã‚¿æƒ…å ±', index=False)
        
    print("\\nâœ… åˆ†æçµæœã‚’ 'analysis_result.xlsx' ã«ä¿å­˜ã—ã¾ã—ãŸ")

# ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–
if sheet_data and len(numeric_cols) > 0:
    plt.figure(figsize=(12, 8))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    n_plots = min(len(numeric_cols), 4)  # æœ€å¤§4ã¤ã®ã‚°ãƒ©ãƒ•
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols[:n_plots]):
        main_df[col].hist(bins=20, ax=axes[i], alpha=0.7, color=f'C{i}')
        axes[i].set_title(f'{col} ã®åˆ†å¸ƒ')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('é »åº¦')
        axes[i].grid(True, alpha=0.3)
    
    # ä½¿ç”¨ã—ãªã„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for i in range(n_plots, 4):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.show()
ã€
```

### ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›

#### JSON â†” CSV å¤‰æ›
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

import json
import pandas as pd
import csv
from datetime import datetime

# 1. JSON to CSV å¤‰æ›
json_data = {
    "employees": [
        {
            "id": 1,
            "name": "ç”°ä¸­å¤ªéƒ",
            "department": "å–¶æ¥­éƒ¨",
            "position": "ä¸»ä»»",
            "salary": 450000,
            "skills": ["å–¶æ¥­", "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³", "Excel"],
            "joined_date": "2020-04-01",
            "performance": {
                "2023": {"sales": 50000000, "rating": "A"},
                "2024": {"sales": 65000000, "rating": "S"}
            }
        },
        {
            "id": 2,
            "name": "ä½è—¤èŠ±å­",
            "department": "é–‹ç™ºéƒ¨",
            "position": "ã‚·ãƒ‹ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
            "salary": 550000,
            "skills": ["Python", "JavaScript", "AWS"],
            "joined_date": "2019-07-15",
            "performance": {
                "2023": {"projects": 8, "rating": "A"},
                "2024": {"projects": 12, "rating": "S"}
            }
        },
        {
            "id": 3,
            "name": "éˆ´æœ¨æ¬¡éƒ",
            "department": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨",
            "position": "ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼",
            "salary": 600000,
            "skills": ["ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "åˆ†æ", "æˆ¦ç•¥ä¼ç”»"],
            "joined_date": "2018-03-01",
            "performance": {
                "2023": {"campaigns": 15, "rating": "A"},
                "2024": {"campaigns": 20, "rating": "S"}
            }
        }
    ]
}

# ãƒã‚¹ãƒˆã—ãŸJSONã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦CSVã«å¤‰æ›
flattened_data = []
for emp in json_data["employees"]:
    row = {
        "id": emp["id"],
        "name": emp["name"],
        "department": emp["department"],
        "position": emp["position"],
        "salary": emp["salary"],
        "skills": "; ".join(emp["skills"]),
        "joined_date": emp["joined_date"],
        "performance_2023": str(emp["performance"]["2023"]),
        "performance_2024": str(emp["performance"]["2024"])
    }
    flattened_data.append(row)

df = pd.DataFrame(flattened_data)
df.to_csv('employees.csv', index=False, encoding='utf-8-sig')
print("âœ… JSON â†’ CSV å¤‰æ›å®Œäº†: employees.csv")
print(df)

# 2. CSV to JSON å¤‰æ›ï¼ˆã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ ï¼‰
# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸJSONã‚’ä½œæˆ
df_read = pd.read_csv('employees.csv')

# éƒ¨ç½²åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦JSONã‚’ä½œæˆ
departments_json = {
    "company": "ã‚µãƒ³ãƒ—ãƒ«æ ªå¼ä¼šç¤¾",
    "update_date": datetime.now().isoformat(),
    "departments": []
}

for dept in df_read['department'].unique():
    dept_employees = df_read[df_read['department'] == dept]
    
    dept_data = {
        "name": dept,
        "employee_count": len(dept_employees),
        "average_salary": int(dept_employees['salary'].mean()),
        "employees": []
    }
    
    for _, emp in dept_employees.iterrows():
        emp_data = {
            "id": int(emp['id']),
            "name": emp['name'],
            "position": emp['position'],
            "salary": int(emp['salary']),
            "skills": emp['skills'].split('; '),
            "joined_date": emp['joined_date'],
            "years_of_service": (datetime.now() - pd.to_datetime(emp['joined_date'])).days // 365
        }
        dept_data["employees"].append(emp_data)
    
    departments_json["departments"].append(dept_data)

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with open('departments.json', 'w', encoding='utf-8') as f:
    json.dump(departments_json, f, ensure_ascii=False, indent=2)

print("\\nâœ… CSV â†’ JSON å¤‰æ›å®Œäº†: departments.json")
print("JSONæ§‹é€ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
print(json.dumps(departments_json, ensure_ascii=False, indent=2)[:500] + "...")

# 3. XMLå¤‰æ›ã‚‚å®Ÿè¡Œ
import xml.etree.ElementTree as ET

# XMLä½œæˆ
root = ET.Element("company")
root.set("name", "ã‚µãƒ³ãƒ—ãƒ«æ ªå¼ä¼šç¤¾")

for dept_data in departments_json["departments"]:
    dept_elem = ET.SubElement(root, "department")
    dept_elem.set("name", dept_data["name"])
    dept_elem.set("employee_count", str(dept_data["employee_count"]))
    
    for emp_data in dept_data["employees"]:
        emp_elem = ET.SubElement(dept_elem, "employee")
        emp_elem.set("id", str(emp_data["id"]))
        
        for key, value in emp_data.items():
            if key != "id":
                if key == "skills":
                    skills_elem = ET.SubElement(emp_elem, "skills")
                    for skill in value:
                        skill_elem = ET.SubElement(skills_elem, "skill")
                        skill_elem.text = skill
                else:
                    elem = ET.SubElement(emp_elem, key)
                    elem.text = str(value)

# XMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
tree = ET.ElementTree(root)
tree.write('employees.xml', encoding='utf-8', xml_declaration=True)
print("\\nâœ… XMLå¤‰æ›ã‚‚å®Œäº†: employees.xml")

# ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
import os
print("\\n=== ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« ===")
for file in ['employees.csv', 'departments.json', 'employees.xml']:
    if os.path.exists(file):
        size = os.path.getsize(file)
        print(f"{file}: {size} bytes")
ã€
```

### ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è‡ªå‹•åŒ–

#### HTMLãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œå£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import base64
from io import BytesIO
from datetime import datetime
import os

# ã‚µãƒ³ãƒ—ãƒ«å£²ä¸Šãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
dates = pd.date_range('2024-01-01', '2024-06-30', freq='D')
products = ['å•†å“A', 'å•†å“B', 'å•†å“C', 'å•†å“D', 'å•†å“E']
regions = ['æ±äº¬', 'å¤§é˜ª', 'åå¤å±‹', 'ç¦å²¡']

sales_data = []
for date in dates:
    for product in products:
        for region in regions:
            if np.random.random() > 0.3:  # 30%ã®ç¢ºç‡ã§ãƒ‡ãƒ¼ã‚¿ãªã—
                sales_data.append({
                    'date': date,
                    'product': product,
                    'region': region,
                    'sales': np.random.gamma(2, 500),
                    'quantity': np.random.poisson(10) + 1,
                    'profit_margin': np.random.uniform(0.1, 0.4)
                })

df = pd.DataFrame(sales_data)
df['profit'] = df['sales'] * df['profit_margin']

print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(df)} ãƒ¬ã‚³ãƒ¼ãƒ‰")

# ã‚°ãƒ©ãƒ•ä½œæˆé–¢æ•°
def create_base64_plot(fig):
    buffer = BytesIO()
    fig.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    plot_data = buffer.getvalue()
    buffer.close()
    plt.close(fig)
    return base64.b64encode(plot_data).decode()

# 1. æœˆåˆ¥å£²ä¸Šæ¨ç§»ã‚°ãƒ©ãƒ•
monthly_sales = df.groupby(df['date'].dt.to_period('M'))['sales'].sum()
fig1, ax1 = plt.subplots(figsize=(12, 6))
monthly_sales.plot(kind='line', ax=ax1, marker='o', linewidth=3, markersize=8)
ax1.set_title('æœˆåˆ¥å£²ä¸Šæ¨ç§»', fontsize=16, fontweight='bold')
ax1.set_xlabel('æœˆ')
ax1.set_ylabel('å£²ä¸Šé‡‘é¡ (å††)')
ax1.grid(True, alpha=0.3)
ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'Â¥{x:,.0f}'))
plot1_base64 = create_base64_plot(fig1)

# 2. å•†å“åˆ¥å£²ä¸Šã‚·ã‚§ã‚¢ï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
product_sales = df.groupby('product')['sales'].sum()
fig2, ax2 = plt.subplots(figsize=(10, 8))
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
wedges, texts, autotexts = ax2.pie(product_sales.values, labels=product_sales.index, 
                                  autopct='%1.1f%%', colors=colors, startangle=90)
ax2.set_title('å•†å“åˆ¥å£²ä¸Šã‚·ã‚§ã‚¢', fontsize=16, fontweight='bold')
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')
plot2_base64 = create_base64_plot(fig2)

# 3. åœ°åŸŸåˆ¥å£²ä¸Šæ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
region_sales = df.groupby('region')['sales'].sum().sort_values(ascending=False)
fig3, ax3 = plt.subplots(figsize=(10, 6))
bars = ax3.bar(region_sales.index, region_sales.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
ax3.set_title('åœ°åŸŸåˆ¥å£²ä¸Šæ¯”è¼ƒ', fontsize=16, fontweight='bold')
ax3.set_xlabel('åœ°åŸŸ')
ax3.set_ylabel('å£²ä¸Šé‡‘é¡ (å††)')
ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'Â¥{x:,.0f}'))

# æ£’ã‚°ãƒ©ãƒ•ã«æ•°å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ 
for bar in bars:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height,
             f'Â¥{height:,.0f}', ha='center', va='bottom', fontweight='bold')

plot3_base64 = create_base64_plot(fig3)

# çµ±è¨ˆãƒ‡ãƒ¼ã‚¿è¨ˆç®—
total_sales = df['sales'].sum()
total_profit = df['profit'].sum()
avg_profit_margin = df['profit_margin'].mean()
best_product = product_sales.idxmax()
best_region = region_sales.idxmax()

# HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
html_content = f'''
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å£²ä¸Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 30px;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
        }}
        .header p {{
            margin: 10px 0 0 0;
            opacity: 0.9;
        }}
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-card {{
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            text-align: center;
            border-left: 5px solid #667eea;
        }}
        .summary-card h3 {{
            margin: 0 0 10px 0;
            color: #667eea;
            font-size: 1.1em;
        }}
        .summary-card .value {{
            font-size: 2em;
            font-weight: bold;
            color: #2c3e50;
            margin: 10px 0;
        }}
        .chart-section {{
            background: white;
            margin: 30px 0;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .chart-section h2 {{
            color: #2c3e50;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }}
        .chart-container {{
            text-align: center;
            margin: 20px 0;
        }}
        .chart-container img {{
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .insights {{
            background: #e8f4fd;
            padding: 25px;
            border-radius: 10px;
            border-left: 5px solid #3498db;
            margin: 30px 0;
        }}
        .insights h3 {{
            color: #2980b9;
            margin-top: 0;
        }}
        .insights ul {{
            margin: 0;
            padding-left: 20px;
        }}
        .insights li {{
            margin: 8px 0;
        }}
        .footer {{
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            color: #7f8c8d;
            border-top: 1px solid #ecf0f1;
        }}
        @media print {{
            body {{ background-color: white; }}
            .chart-section {{ page-break-inside: avoid; }}
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“Š å£²ä¸Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>æœŸé–“: 2024å¹´1æœˆ1æ—¥ ã€œ 2024å¹´6æœˆ30æ—¥</p>
        <p>ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}</p>
    </div>

    <div class="summary">
        <div class="summary-card">
            <h3>ğŸ“ˆ ç·å£²ä¸Š</h3>
            <div class="value">Â¥{total_sales:,.0f}</div>
        </div>
        <div class="summary-card">
            <h3>ğŸ’° ç·åˆ©ç›Š</h3>
            <div class="value">Â¥{total_profit:,.0f}</div>
        </div>
        <div class="summary-card">
            <h3>ğŸ“Š å¹³å‡åˆ©ç›Šç‡</h3>
            <div class="value">{avg_profit_margin:.1%}</div>
        </div>
        <div class="summary-card">
            <h3>ğŸ† æœ€å„ªç§€å•†å“</h3>
            <div class="value">{best_product}</div>
        </div>
    </div>

    <div class="chart-section">
        <h2>ğŸ“ˆ æœˆåˆ¥å£²ä¸Šæ¨ç§»</h2>
        <div class="chart-container">
            <img src="data:image/png;base64,{plot1_base64}" alt="æœˆåˆ¥å£²ä¸Šæ¨ç§»">
        </div>
    </div>

    <div class="chart-section">
        <h2>ğŸ¥§ å•†å“åˆ¥å£²ä¸Šã‚·ã‚§ã‚¢</h2>
        <div class="chart-container">
            <img src="data:image/png;base64,{plot2_base64}" alt="å•†å“åˆ¥å£²ä¸Šã‚·ã‚§ã‚¢">
        </div>
    </div>

    <div class="chart-section">
        <h2>ğŸ—¾ åœ°åŸŸåˆ¥å£²ä¸Šæ¯”è¼ƒ</h2>
        <div class="chart-container">
            <img src="data:image/png;base64,{plot3_base64}" alt="åœ°åŸŸåˆ¥å£²ä¸Šæ¯”è¼ƒ">
        </div>
    </div>

    <div class="insights">
        <h3>ğŸ’¡ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ</h3>
        <ul>
            <li><strong>æœ€å„ªç§€åœ°åŸŸ:</strong> {best_region}ãŒæœ€ã‚‚é«˜ã„å£²ä¸Šã‚’è¨˜éŒ²</li>
            <li><strong>ãƒˆãƒƒãƒ—å•†å“:</strong> {best_product}ãŒå£²ä¸Šã‚’ãƒªãƒ¼ãƒ‰</li>
            <li><strong>åˆ©ç›Šç‡:</strong> å¹³å‡{avg_profit_margin:.1%}ã®å¥å…¨ãªåˆ©ç›Šç‡ã‚’ç¶­æŒ</li>
            <li><strong>æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰:</strong> æœˆæ¬¡å£²ä¸Šã¯å®‰å®šã—ãŸæˆé•·ã‚’ç¤ºã—ã¦ã„ã‚‹</li>
            <li><strong>åœ°åŸŸãƒãƒ©ãƒ³ã‚¹:</strong> å…¨åœ°åŸŸã§å‡ç­‰ãªå£²ä¸Šåˆ†å¸ƒã‚’å®Ÿç¾</li>
        </ul>
    </div>

    <div class="footer">
        <p>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ | æ©Ÿå¯†æƒ…å ± - ç¤¾å¤–ç§˜</p>
    </div>
</body>
</html>
'''

# HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
with open('sales_report.html', 'w', encoding='utf-8') as f:
    f.write(html_content)

print("âœ… HTMLãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: sales_report.html")
print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize('sales_report.html')} bytes")

# ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚‚è¡¨ç¤º
print("\\n=== ãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ ===")
print(f"ç·å£²ä¸Š: Â¥{total_sales:,.0f}")
print(f"ç·åˆ©ç›Š: Â¥{total_profit:,.0f}")
print(f"å¹³å‡åˆ©ç›Šç‡: {avg_profit_margin:.1%}")
print(f"æœ€å„ªç§€å•†å“: {best_product}")
print(f"æœ€å„ªç§€åœ°åŸŸ: {best_region}")
print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['date'].min().strftime('%Y-%m-%d')} ã€œ {df['date'].max().strftime('%Y-%m-%d')}")
ã€
```

---

## é«˜åº¦ãªæ´»ç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### ğŸ”§ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ‡ãƒãƒƒã‚°

#### å …ç‰¢ãªã‚³ãƒ¼ãƒ‰ä½œæˆ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å«ã‚€å …ç‰¢ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import numpy as np
import logging
from datetime import datetime
import traceback
import warnings

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataProcessor:
    def __init__(self):
        self.processed_data = None
        self.errors = []
        self.warnings = []
        
    def validate_data(self, df):
        \"\"\"ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯\"\"\"
        issues = []
        
        try:
            # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
            if df.empty:
                issues.append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return issues
            
            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            missing_data = df.isnull().sum()
            if missing_data.sum() > 0:
                missing_cols = missing_data[missing_data > 0]
                issues.append(f"æ¬ æå€¤ç™ºè¦‹: {dict(missing_cols)}")
            
            # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            for col in df.columns:
                if df[col].dtype == 'object':
                    # æ•°å€¤ã¨ã—ã¦è§£é‡ˆå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                    try:
                        pd.to_numeric(df[col], errors='raise')
                        issues.append(f"åˆ—'{col}'ã¯æ•°å€¤å‹ã«å¤‰æ›å¯èƒ½ã§ã™")
                    except (ValueError, TypeError):
                        pass
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                issues.append(f"é‡è¤‡è¡Œ: {duplicates}ä»¶")
            
            # å¤–ã‚Œå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                if len(outliers) > 0:
                    issues.append(f"åˆ—'{col}'ã«å¤–ã‚Œå€¤: {len(outliers)}ä»¶")
            
            return issues
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return [f"æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}"]
    
    def clean_data(self, df):
        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\"\"\"
        try:
            logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
            original_shape = df.shape
            
            # 1. é‡è¤‡è¡Œã®å‰Šé™¤
            df_cleaned = df.drop_duplicates()
            if df_cleaned.shape[0] != original_shape[0]:
                removed_duplicates = original_shape[0] - df_cleaned.shape[0]
                logger.info(f"é‡è¤‡è¡Œ {removed_duplicates}ä»¶ã‚’å‰Šé™¤")
            
            # 2. æ¬ æå€¤ã®å‡¦ç†
            for col in df_cleaned.columns:
                missing_count = df_cleaned[col].isnull().sum()
                if missing_count > 0:
                    if df_cleaned[col].dtype in ['int64', 'float64']:
                        # æ•°å€¤åˆ—ã¯ä¸­å¤®å€¤ã§è£œå®Œ
                        median_val = df_cleaned[col].median()
                        df_cleaned[col].fillna(median_val, inplace=True)
                        logger.info(f"åˆ—'{col}': æ¬ æå€¤{missing_count}ä»¶ã‚’ä¸­å¤®å€¤{median_val}ã§è£œå®Œ")
                    else:
                        # ã‚«ãƒ†ã‚´ãƒªåˆ—ã¯æœ€é »å€¤ã§è£œå®Œ
                        mode_val = df_cleaned[col].mode()
                        if len(mode_val) > 0:
                            df_cleaned[col].fillna(mode_val[0], inplace=True)
                            logger.info(f"åˆ—'{col}': æ¬ æå€¤{missing_count}ä»¶ã‚’æœ€é »å€¤'{mode_val[0]}'ã§è£œå®Œ")
                        else:
                            df_cleaned[col].fillna('Unknown', inplace=True)
                            logger.info(f"åˆ—'{col}': æ¬ æå€¤{missing_count}ä»¶ã‚’'Unknown'ã§è£œå®Œ")
            
            # 3. ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–
            for col in df_cleaned.columns:
                if df_cleaned[col].dtype == 'object':
                    # æ•°å€¤å¤‰æ›ã‚’è©¦è¡Œ
                    try:
                        numeric_series = pd.to_numeric(df_cleaned[col], errors='coerce')
                        if not numeric_series.isnull().all():
                            # æ•´æ•°ã‹æµ®å‹•å°æ•°ç‚¹ã‹ã‚’åˆ¤å®š
                            if numeric_series.equals(numeric_series.astype('int64', errors='ignore')):
                                df_cleaned[col] = numeric_series.astype('int64')
                                logger.info(f"åˆ—'{col}'ã‚’int64ã«å¤‰æ›")
                            else:
                                df_cleaned[col] = numeric_series
                                logger.info(f"åˆ—'{col}'ã‚’float64ã«å¤‰æ›")
                    except Exception as e:
                        logger.debug(f"åˆ—'{col}'ã®æ•°å€¤å¤‰æ›å¤±æ•—: {e}")
                        # ã‚«ãƒ†ã‚´ãƒªåŒ–ã‚’æ¤œè¨
                        unique_ratio = df_cleaned[col].nunique() / len(df_cleaned)
                        if unique_ratio < 0.5:  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®å‰²åˆãŒ50%æœªæº€
                            df_cleaned[col] = df_cleaned[col].astype('category')
                            logger.info(f"åˆ—'{col}'ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›")
            
            logger.info(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {original_shape} â†’ {df_cleaned.shape}")
            return df_cleaned
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return df
    
    def analyze_data(self, df):
        \"\"\"ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œ\"\"\"
        results = {}
        
        try:
            logger.info("ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")
            
            # åŸºæœ¬æƒ…å ±
            results['basic_info'] = {
                'shape': df.shape,
                'columns': list(df.columns),
                'dtypes': df.dtypes.to_dict(),
                'memory_usage': df.memory_usage(deep=True).sum()
            }
            
            # æ•°å€¤åˆ—ã®çµ±è¨ˆ
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                results['numeric_stats'] = df[numeric_cols].describe().to_dict()
                
                # ç›¸é–¢åˆ†æ
                if len(numeric_cols) > 1:
                    correlation_matrix = df[numeric_cols].corr()
                    results['correlations'] = correlation_matrix.to_dict()
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®åˆ†æ
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            if len(categorical_cols) > 0:
                results['categorical_stats'] = {}
                for col in categorical_cols:
                    results['categorical_stats'][col] = {
                        'unique_count': df[col].nunique(),
                        'top_values': df[col].value_counts().head(5).to_dict()
                    }
            
            logger.info("ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†")
            return results
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return {'error': str(e)}
    
    def process_file(self, file_path):
        \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼\"\"\"
        try:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            try:
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path, encoding='utf-8')
                elif file_path.endswith('.xlsx'):
                    df = pd.read_excel(file_path)
                elif file_path.endswith('.json'):
                    df = pd.read_json(file_path)
                else:
                    raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_path}")
                
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {df.shape}")
                
            except UnicodeDecodeError:
                logger.warning("UTF-8ã§ã®èª­ã¿è¾¼ã¿å¤±æ•—ã€Shift_JISã§å†è©¦è¡Œ")
                df = pd.read_csv(file_path, encoding='shift_jis')
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                raise
            
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            validation_issues = self.validate_data(df)
            if validation_issues:
                logger.warning("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§å•é¡Œã‚’ç™ºè¦‹:")
                for issue in validation_issues:
                    logger.warning(f"  - {issue}")
                self.warnings.extend(validation_issues)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            df_cleaned = self.clean_data(df)
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†æ
            analysis_results = self.analyze_data(df_cleaned)
            
            # çµæœä¿å­˜
            self.processed_data = df_cleaned
            
            # çµæœã‚µãƒãƒªãƒ¼
            summary = {
                'file_path': file_path,
                'original_shape': df.shape,
                'processed_shape': df_cleaned.shape,
                'processing_time': datetime.now().isoformat(),
                'warnings': self.warnings,
                'analysis': analysis_results
            }
            
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†")
            return summary
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            self.errors.append(str(e))
            return {'error': str(e), 'traceback': traceback.format_exc()}

# ä½¿ç”¨ä¾‹
processor = DataProcessor()

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
test_data = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5, 1],  # é‡è¤‡ã¨æ¬ æå€¤
    'B': ['x', 'y', 'z', 'x', np.nan, 'x'],  # ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿
    'C': [10.5, 20.0, 30.0, 40.0, 1000.0, 50.0],  # å¤–ã‚Œå€¤ã‚ã‚Š
    'D': ['2024-01-01', '2024-01-02', 'invalid', '2024-01-04', '2024-01-05', '2024-01-06']
})

test_data.to_csv('test_data.csv', index=False)

# å‡¦ç†å®Ÿè¡Œ
result = processor.process_file('test_data.csv')

print("=== å‡¦ç†çµæœ ===")
if 'error' in result:
    print(f"ã‚¨ãƒ©ãƒ¼: {result['error']}")
else:
    print(f"å…ƒãƒ‡ãƒ¼ã‚¿: {result['original_shape']}")
    print(f"å‡¦ç†å¾Œ: {result['processed_shape']}")
    print(f"è­¦å‘Šæ•°: {len(result['warnings'])}")
    
    if result['warnings']:
        print("\\nè­¦å‘Šå†…å®¹:")
        for warning in result['warnings']:
            print(f"  - {warning}")
    
    print(f"\\nå‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿:")
    print(processor.processed_data.head())
    print(f"\\nãƒ‡ãƒ¼ã‚¿å‹:")
    print(processor.processed_data.dtypes)
ã€
```

### ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
ã€Œå¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã®æœ€é©åŒ–æŠ€è¡“ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

import pandas as pd
import numpy as np
import time
from functools import wraps
import psutil
import gc
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp

def performance_monitor(func):
    \"\"\"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿\"\"\"
    @wraps(func)
    def wrapper(*args, **kwargs):
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå®Ÿè¡Œå‰ï¼‰
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå®Ÿè¡Œå¾Œï¼‰
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"\\n=== {func.__name__} ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ===")
        print(f"å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_before:.1f}MB â†’ {memory_after:.1f}MB")
        print(f"ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_after - memory_before:.1f}MB")
        
        return result
    return wrapper

class OptimizedDataProcessor:
    def __init__(self):
        self.chunk_size = 10000
        self.n_cores = mp.cpu_count()
    
    @performance_monitor
    def create_large_dataset(self, n_rows=1000000):
        \"\"\"å¤§é‡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ\"\"\"
        print(f"å¤§é‡ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­... ({n_rows:,}è¡Œ)")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        
        data = {
            'id': range(1, n_rows + 1),
            'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),
            'value1': np.random.normal(100, 15, n_rows),
            'value2': np.random.exponential(50, n_rows),
            'date': pd.date_range('2020-01-01', periods=n_rows, freq='H'),
            'flag': np.random.choice([True, False], n_rows, p=[0.3, 0.7])
        }
        
        df = pd.DataFrame(data)
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        df = self.optimize_dtypes(df)
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
        return df
    
    def optimize_dtypes(self, df):
        \"\"\"ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–\"\"\"
        original_memory = df.memory_usage(deep=True).sum()
        
        for col in df.columns:
            col_type = df[col].dtype
            
            if col_type != 'object':
                c_min = df[col].min()
                c_max = df[col].max()
                
                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                
                elif str(col_type)[:5] == 'float':
                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
            
            elif col_type == 'object':
                # ã‚«ãƒ†ã‚´ãƒªåŒ–ã®æ¤œè¨
                unique_ratio = df[col].nunique() / len(df)
                if unique_ratio < 0.5:
                    df[col] = df[col].astype('category')
        
        optimized_memory = df.memory_usage(deep=True).sum()
        reduction = (1 - optimized_memory / original_memory) * 100
        
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›: {reduction:.1f}%")
        return df
    
    @performance_monitor
    def chunked_processing(self, df, operation='sum'):
        \"\"\"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†\"\"\"
        print(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–‹å§‹ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self.chunk_size:,})")
        
        results = []
        n_chunks = len(df) // self.chunk_size + (1 if len(df) % self.chunk_size != 0 else 0)
        
        for i in range(0, len(df), self.chunk_size):
            chunk = df.iloc[i:i + self.chunk_size]
            
            if operation == 'sum':
                result = chunk.select_dtypes(include=[np.number]).sum()
            elif operation == 'mean':
                result = chunk.select_dtypes(include=[np.number]).mean()
            elif operation == 'group_sum':
                result = chunk.groupby('category')['value1'].sum()
            
            results.append(result)
            
            if (i // self.chunk_size + 1) % 10 == 0:
                print(f"é€²æ—: {i // self.chunk_size + 1}/{n_chunks} ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†")
        
        # çµæœçµ±åˆ
        if operation in ['sum', 'mean']:
            final_result = pd.concat(results, axis=1).sum(axis=1)
        elif operation == 'group_sum':
            final_result = pd.concat(results).groupby('category').sum()
        
        return final_result
    
    def parallel_chunk_processor(self, chunk):
        \"\"\"ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°\"\"\"
        return chunk.groupby('category').agg({
            'value1': ['sum', 'mean', 'count'],
            'value2': ['sum', 'mean']
        })
    
    @performance_monitor
    def parallel_processing(self, df):
        \"\"\"ä¸¦åˆ—å‡¦ç†\"\"\"
        print(f"ä¸¦åˆ—å‡¦ç†é–‹å§‹ (ã‚³ã‚¢æ•°: {self.n_cores})")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [df.iloc[i:i + self.chunk_size] for i in range(0, len(df), self.chunk_size)]
        
        # ProcessPoolExecutorã§ä¸¦åˆ—å‡¦ç†
        with ProcessPoolExecutor(max_workers=self.n_cores) as executor:
            results = list(executor.map(self.parallel_chunk_processor, chunks))
        
        # çµæœçµ±åˆ
        combined_result = pd.concat(results)
        final_result = combined_result.groupby('category').agg({
            ('value1', 'sum'): 'sum',
            ('value1', 'mean'): 'mean',
            ('value1', 'count'): 'sum',
            ('value2', 'sum'): 'sum',
            ('value2', 'mean'): 'mean'
        })
        
        return final_result
    
    @performance_monitor
    def vectorized_operations(self, df):
        \"\"\"ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—\"\"\"
        print("ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—å®Ÿè¡Œ")
        
        # è¤‡é›‘ãªè¨ˆç®—ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        # å¾“æ¥ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ã®ä»£ã‚ã‚Šã«ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã‚’ä½¿ç”¨
        
        # æ¡ä»¶ã«åŸºã¥ãã‚¹ã‚³ã‚¢è¨ˆç®—
        conditions = [
            (df['value1'] > 100) & (df['value2'] > 50),
            (df['value1'] > 80) & (df['value2'] > 30),
            (df['value1'] > 60) & (df['value2'] > 20)
        ]
        choices = [100, 80, 60]
        
        df['score'] = np.select(conditions, choices, default=0)
        
        # è¤‡é›‘ãªæ•°å¼è¨ˆç®—
        df['complex_metric'] = (
            np.log1p(df['value1']) * 
            np.sqrt(df['value2']) * 
            np.where(df['flag'], 1.2, 1.0)
        )
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆã®é«˜é€Ÿè¨ˆç®—
        category_stats = df.groupby('category').agg({
            'value1': ['sum', 'mean', 'std'],
            'value2': ['sum', 'mean', 'std'],
            'score': 'mean',
            'complex_metric': 'mean'
        })
        
        return category_stats
    
    @performance_monitor
    def memory_efficient_join(self, df1, df2):
        \"\"\"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªçµåˆ\"\"\"
        print("ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªçµåˆå®Ÿè¡Œ")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æœ€é©åŒ–
        df1_indexed = df1.set_index('id')
        df2_indexed = df2.set_index('id')
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«çµåˆ
        results = []
        for i in range(0, len(df1_indexed), self.chunk_size):
            chunk1 = df1_indexed.iloc[i:i + self.chunk_size]
            
            # å¯¾å¿œã™ã‚‹IDã®ã¿ã‚’df2ã‹ã‚‰æŠ½å‡º
            matching_ids = chunk1.index.intersection(df2_indexed.index)
            chunk2 = df2_indexed.loc[matching_ids]
            
            # çµåˆ
            joined_chunk = chunk1.join(chunk2, how='inner')
            results.append(joined_chunk)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            del chunk1, chunk2, joined_chunk
            gc.collect()
        
        final_result = pd.concat(results)
        return final_result

# ä½¿ç”¨ä¾‹ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
processor = OptimizedDataProcessor()

# 1. å¤§é‡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
large_df = processor.create_large_dataset(500000)

# 2. é€šå¸¸å‡¦ç† vs ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®æ¯”è¼ƒ
print("\\n" + "="*50)
print("å‡¦ç†æ–¹æ³•ã®æ¯”è¼ƒ")
print("="*50)

# é€šå¸¸ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
@performance_monitor
def normal_grouping(df):
    return df.groupby('category')['value1'].sum()

# ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
result_normal = normal_grouping(large_df)
result_chunked = processor.chunked_processing(large_df, 'group_sum')

print("\\nçµæœæ¯”è¼ƒ:")
print("é€šå¸¸å‡¦ç†:")
print(result_normal)
print("\\nãƒãƒ£ãƒ³ã‚¯å‡¦ç†:")
print(result_chunked)

# 3. ä¸¦åˆ—å‡¦ç†
result_parallel = processor.parallel_processing(large_df)
print("\\nä¸¦åˆ—å‡¦ç†çµæœ:")
print(result_parallel)

# 4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—
result_vectorized = processor.vectorized_operations(large_df.copy())
print("\\nãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—çµæœ:")
print(result_vectorized.head())

# 5. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
print("\\n" + "="*50)
print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†æ")
print("="*50)

process = psutil.Process()
memory_info = process.memory_info()
print(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_info.rss / 1024 / 1024:.1f}MB")
print(f"ä»®æƒ³ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_info.vms / 1024 / 1024:.1f}MB")

# ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
gc.collect()
memory_after_gc = process.memory_info()
print(f"GCå¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_after_gc.rss / 1024 / 1024:.1f}MB")

# 6. æœ€é©åŒ–ã®ã¾ã¨ã‚
print("\\n" + "="*50)
print("æœ€é©åŒ–æ‰‹æ³•ã¾ã¨ã‚")
print("="*50)
print("âœ… ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›")
print("âœ… ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å‡¦ç†")
print("âœ… ä¸¦åˆ—å‡¦ç†: è¤‡æ•°ã‚³ã‚¢ã‚’æ´»ç”¨ã—ã¦é«˜é€ŸåŒ–")
print("âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—: NumPyã®é«˜é€Ÿæ¼”ç®—ã‚’æ´»ç”¨")
print("âœ… ãƒ¡ãƒ¢ãƒªç®¡ç†: å®šæœŸçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
ã€
```

---

## ğŸ’¡ å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’1: åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿åˆ†æ

ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’ChatGPT Code Interpreterã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```
ã‚¿ã‚¹ã‚¯: å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®åˆ†æ

ãƒ‡ãƒ¼ã‚¿ä½œæˆ:
- 6ã‹æœˆåˆ†ã®æ—¥åˆ¥å£²ä¸Šãƒ‡ãƒ¼ã‚¿
- 5ã¤ã®å•†å“ã‚«ãƒ†ã‚´ãƒª
- 3ã¤ã®åœ°åŸŸ
- å£²ä¸Šé‡‘é¡ã€æ•°é‡ã€åˆ©ç›Šç‡

åˆ†æå†…å®¹:
1. åŸºæœ¬çµ±è¨ˆæƒ…å ±ã®ç®—å‡º
2. æœˆåˆ¥ãƒ»ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»åœ°åŸŸåˆ¥ã®é›†è¨ˆ
3. ç›¸é–¢åˆ†æ
4. å¯è¦–åŒ–ï¼ˆ3ç¨®é¡ä»¥ä¸Šã®ã‚°ãƒ©ãƒ•ï¼‰
5. ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®æŠ½å‡º
```

### æ¼”ç¿’2: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ»å¤‰æ›

ä»¥ä¸‹ã®å¤‰æ›ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```
ã‚¿ã‚¹ã‚¯: è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–“ã®å¤‰æ›

å‡¦ç†å†…å®¹:
1. JSON â†’ CSVå¤‰æ›
2. Excel â†’ JSONå¤‰æ›
3. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ¤œè¨¼
4. HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å®Ÿè£…

è¦ä»¶:
- å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
- ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼
- å‡¦ç†ãƒ­ã‚°ã®å‡ºåŠ›
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```

### æ¼”ç¿’3: é«˜åº¦ãªå¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ

ä»¥ä¸‹ã®é«˜åº¦ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

```
ã‚¿ã‚¹ã‚¯: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

æ©Ÿèƒ½:
1. Plotlyã‚’ä½¿ã£ãŸå‹•çš„ã‚°ãƒ©ãƒ•
2. è¤‡æ•°ã®å¯è¦–åŒ–å½¢å¼
3. HTMLãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ
4. çµ±è¨ˆåˆ†æçµæœã®çµ„ã¿è¾¼ã¿
5. ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒ‡ã‚¶ã‚¤ãƒ³

æŠ€è¡“è¦ä»¶:
- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ
- å°åˆ·æœ€é©åŒ–
- ãƒ‡ãƒ¼ã‚¿æ›´æ–°å¯¾å¿œ
- ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

### ChatGPT Code Interpreter
- [OpenAI Code Interpreter Guide](https://help.openai.com/en/articles/8437071-code-interpreter)
- [Advanced Data Analysis](https://openai.com/blog/chatgpt-plugins#code-interpreter)

### Python ãƒ‡ãƒ¼ã‚¿åˆ†æ
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [NumPy Documentation](https://numpy.org/doc/)
- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)
- [Plotly Python](https://plotly.com/python/)

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- [Pandas Performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
- [Python Profiling](https://docs.python.org/3/library/profile.html)

---

## âœ… ç†è§£åº¦ç¢ºèª

ã“ã®è¬›åº§ã®å†…å®¹ã‚’ç†è§£ã§ããŸã‹ã€ä»¥ä¸‹ã§ç¢ºèªã—ã¦ãã ã•ã„ï¼š

1. ChatGPT Code Interpreterã®åŸºæœ¬æ“ä½œã‚’ç¿’å¾—ã—ã¾ã—ãŸã‹ï¼Ÿ
2. ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–ã‚’åŠ¹æœçš„ã«å®Ÿè¡Œã§ãã¾ã™ã‹ï¼Ÿ
3. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ»å¤‰æ›ä½œæ¥­ã‚’è‡ªå‹•åŒ–ã§ãã¾ã™ã‹ï¼Ÿ
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°æŠ€è¡“ã‚’ç†è§£ã—ã¾ã—ãŸã‹ï¼Ÿ
5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ‰‹æ³•ã‚’é©ç”¨ã§ãã¾ã™ã‹ï¼Ÿ

ã™ã¹ã¦ã€Œã¯ã„ã€ãªã‚‰æ¬¡ã®è¬›åº§ã«é€²ã‚ã¾ã™ã€‚

---

**æ¬¡ã®è¬›åº§**: [Claudeã§ã®Pythonå®Ÿè¡Œ](12-python-claude.md)