# ãã®ä»–LLMã§ã®Pythonå®Ÿè¡Œ

**æ‰€è¦æ™‚é–“**: 1æ™‚é–“  
**ãƒ¬ãƒ™ãƒ«**: ä¸­ç´š  
**å‰æçŸ¥è­˜**: [ChatGPTã§ã®Pythonå®Ÿè¡Œ](11-python-chatgpt.md), [Claudeã§ã®Pythonå®Ÿè¡Œ](12-python-claude.md)

## å­¦ç¿’ç›®æ¨™

ã“ã®è¬›åº§ã‚’ä¿®äº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š
- GitHub Copilotã€Google Geminiã€Microsoft Copilotã§ã®ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã‚’ç†è§£ã§ãã‚‹
- å„LLMã®ç‰¹å¾´ã‚’æ´»ã‹ã—ãŸä½¿ã„åˆ†ã‘ãŒã§ãã‚‹
- è¤‡æ•°ã®LLMã‚’çµ„ã¿åˆã‚ã›ãŸé–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã§ãã‚‹
- ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸé¸æŠãŒã§ãã‚‹

## ğŸ“‹ ç›®æ¬¡

1. [å„LLMã®Pythonå®Ÿè¡Œç’°å¢ƒæ¯”è¼ƒ](#å„llmã®pythonå®Ÿè¡Œç’°å¢ƒæ¯”è¼ƒ)
2. [GitHub Copilotæ´»ç”¨](#github-copilotæ´»ç”¨)
3. [Google Geminiå®Ÿè¡Œç’°å¢ƒ](#google-geminiå®Ÿè¡Œç’°å¢ƒ)
4. [Microsoft Copilotæ´»ç”¨](#microsoft-copilotæ´»ç”¨)
5. [LLMçµ„ã¿åˆã‚ã›æˆ¦ç•¥](#llmçµ„ã¿åˆã‚ã›æˆ¦ç•¥)

---

## å„LLMã®Pythonå®Ÿè¡Œç’°å¢ƒæ¯”è¼ƒ

### ğŸ”„ ä¸»è¦LLMã®ç‰¹å¾´æ¯”è¼ƒ

| ç‰¹å¾´ | ChatGPT | Claude | GitHub Copilot | Google Gemini | MS Copilot |
|------|---------|--------|---------------|--------------|------------|
| **Pythonå®Ÿè¡Œ** | âœ… å†…è”µ | âœ… Artifacts | âŒ IDEé€£æº | âœ… åˆ¶é™ã‚ã‚Š | âœ… åˆ¶é™ã‚ã‚Š |
| **ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†** | âœ… å¼·åŠ› | åˆ¶é™ã‚ã‚Š | âœ… ãƒ­ãƒ¼ã‚«ãƒ« | åˆ¶é™ã‚ã‚Š | åˆ¶é™ã‚ã‚Š |
| **ã‚³ãƒ¼ãƒ‰å“è³ª** | 90.2% | 93.7% | 95%+ | 71.9% | 90%+ |
| **ã‚³ã‚¹ãƒˆ** | é«˜ | é«˜ | ä½ã€œä¸­ | æœ€å®‰ | ä¸­ |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±** | âŒ | âŒ | âŒ | âœ… | âœ… |
| **çµ±åˆé–‹ç™ºç’°å¢ƒ** | âŒ | âŒ | âœ… æœ€å¼· | âŒ | âœ… Office |

### ğŸ¯ ç”¨é€”åˆ¥æ¨å¥¨LLM

#### ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–
```
1ä½: ChatGPT Code Interpreter - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã¨å¯è¦–åŒ–ãŒå¼·åŠ›
2ä½: Claude - çµ±è¨ˆçš„å³å¯†æ€§ã¨è©³ç´°è§£èª¬
3ä½: Gemini - ã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–ãªã‚‰æœ€é©
4ä½: MS Copilot - Officeé€£æºãŒå¿…è¦ãªå ´åˆ
```

#### ã‚³ãƒ¼ãƒ‰é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°
```
1ä½: GitHub Copilot - IDEçµ±åˆã§æœ€é«˜ã®é–‹ç™ºä½“é¨“
2ä½: Claude - é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨è§£èª¬
3ä½: ChatGPT - ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã¨å®Ÿé¨“
4ä½: Gemini - åŸºæœ¬çš„ãªé–‹ç™ºã‚µãƒãƒ¼ãƒˆ
```

#### å­¦ç¿’ãƒ»æ•™è‚²
```
1ä½: Claude - è©³ç´°ãªè§£èª¬ã¨ç†è«–çš„èƒŒæ™¯
2ä½: ChatGPT - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå­¦ç¿’
3ä½: GitHub Copilot - å®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
4ä½: Gemini - åŸºç¤å­¦ç¿’
```

---

## GitHub Copilotæ´»ç”¨

### ğŸ’» çµ±åˆé–‹ç™ºç’°å¢ƒã§ã®æ´»ç”¨

#### VS Code ã§ã®åŸºæœ¬è¨­å®š
```python
# GitHub Copilot ã‚’æ´»ç”¨ã—ãŸPythoné–‹ç™º

# 1. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚µã‚¸ã‚§ã‚¹ãƒˆæ´»ç”¨
def calculate_sales_metrics(sales_data):
    """
    å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    Copilot ãŒè‡ªå‹•çš„ã«ã‚³ãƒ¼ãƒ‰ã‚’ææ¡ˆ
    """
    # Copilotææ¡ˆ: åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
    total_sales = sum(sales_data)
    average_sales = total_sales / len(sales_data)
    max_sales = max(sales_data)
    min_sales = min(sales_data)
    
    # Copilotææ¡ˆ: æ¨™æº–åå·®ã®è¨ˆç®—
    import statistics
    std_dev = statistics.stdev(sales_data)
    
    return {
        'total': total_sales,
        'average': average_sales,
        'max': max_sales,
        'min': min_sales,
        'std_dev': std_dev
    }

# 2. ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã§ã®è©³ç´°èª¬æ˜è¦æ±‚
# Ctrl+I ã§ Copilot Chat ã‚’é–‹ãã€ä»¥ä¸‹ã®ã‚ˆã†ã«è³ªå•:
# "ã“ã®é–¢æ•°ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ ã—ã¦"
# "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã—ã¦"
# "å˜ä½“ãƒ†ã‚¹ãƒˆã‚’ä½œæˆã—ã¦"
```

#### Copilot Chat ã§ã®é«˜åº¦ãªæ´»ç”¨
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆCopilot Chatå†…ï¼‰
"""
ä»¥ä¸‹ã®è¦ä»¶ã§ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

1. CSVã€Excelã€JSON ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¯¾å¿œ
2. åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—
3. å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆmatplotlibä½¿ç”¨ï¼‰
4. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
6. ãƒ­ã‚°æ©Ÿèƒ½
7. å‹ãƒ’ãƒ³ãƒˆå¯¾å¿œ
8. docstringå®Œå‚™

ã‚¯ãƒ©ã‚¹å: DataAnalyzer
ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰: load_data, analyze, visualize, export
"""

# Copilot ãŒç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹
from typing import Dict, List, Optional, Union
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
from pathlib import Path

class DataAnalyzer:
    """
    ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’è¡Œã†ãŸã‚ã®ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, log_level: str = "INFO"):
        """
        DataAnalyzer ã®åˆæœŸåŒ–
        
        Args:
            log_level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
        """
        self.data: Optional[pd.DataFrame] = None
        self.analysis_results: Dict = {}
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self, file_path: Union[str, Path], **kwargs) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            **kwargs: pandasèª­ã¿è¾¼ã¿é–¢æ•°ã¸ã®è¿½åŠ å¼•æ•°
            
        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸã®å¯å¦
        """
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ãŸèª­ã¿è¾¼ã¿
            suffix = file_path.suffix.lower()
            
            if suffix == '.csv':
                self.data = pd.read_csv(file_path, **kwargs)
            elif suffix in ['.xlsx', '.xls']:
                self.data = pd.read_excel(file_path, **kwargs)
            elif suffix == '.json':
                self.data = pd.read_json(file_path, **kwargs)
            else:
                raise ValueError(f"Unsupported file format: {suffix}")
            
            self.logger.info(f"Data loaded successfully. Shape: {self.data.shape}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error loading data: {e}")
            return False
    
    def analyze(self) -> Dict:
        """
        ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œ
        
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        if self.data is None:
            raise ValueError("No data loaded. Call load_data() first.")
        
        try:
            # åŸºæœ¬çµ±è¨ˆ
            basic_stats = self.data.describe()
            
            # æ¬ æå€¤æƒ…å ±
            missing_info = self.data.isnull().sum()
            
            # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±
            dtype_info = self.data.dtypes
            
            # ç›¸é–¢åˆ†æï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            numeric_cols = self.data.select_dtypes(include=[np.number]).columns
            correlation_matrix = self.data[numeric_cols].corr() if len(numeric_cols) > 1 else None
            
            self.analysis_results = {
                'basic_stats': basic_stats,
                'missing_values': missing_info,
                'data_types': dtype_info,
                'correlation_matrix': correlation_matrix,
                'shape': self.data.shape
            }
            
            self.logger.info("Analysis completed successfully")
            return self.analysis_results
            
        except Exception as e:
            self.logger.error(f"Error during analysis: {e}")
            raise
    
    def visualize(self, plot_type: str = "histogram", 
                  columns: Optional[List[str]] = None) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–
        
        Args:
            plot_type: ãƒ—ãƒ­ãƒƒãƒˆç¨®é¡
            columns: å¯¾è±¡åˆ—ï¼ˆNoneã®å ´åˆã¯æ•°å€¤åˆ—å…¨ã¦ï¼‰
        """
        if self.data is None:
            raise ValueError("No data loaded")
        
        if columns is None:
            columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not columns:
            raise ValueError("No numeric columns found")
        
        try:
            fig, axes = plt.subplots(
                nrows=(len(columns) + 1) // 2, 
                ncols=2, 
                figsize=(12, 6 * ((len(columns) + 1) // 2))
            )
            
            if len(columns) == 1:
                axes = [axes]
            elif len(columns) > 2:
                axes = axes.flatten()
            
            for i, col in enumerate(columns):
                ax = axes[i] if len(columns) > 1 else axes
                
                if plot_type == "histogram":
                    self.data[col].hist(bins=30, ax=ax, alpha=0.7)
                elif plot_type == "boxplot":
                    self.data.boxplot(column=col, ax=ax)
                elif plot_type == "scatter" and len(columns) >= 2:
                    if i < len(columns) - 1:
                        ax.scatter(self.data[columns[i]], self.data[columns[i+1]])
                        ax.set_xlabel(columns[i])
                        ax.set_ylabel(columns[i+1])
                
                ax.set_title(f'{plot_type.title()} of {col}')
            
            # ä½¿ç”¨ã—ãªã„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
            if len(columns) % 2 == 1 and len(columns) > 1:
                axes[-1].set_visible(False)
            
            plt.tight_layout()
            plt.show()
            
            self.logger.info(f"Visualization created: {plot_type}")
            
        except Exception as e:
            self.logger.error(f"Error creating visualization: {e}")
            raise
    
    def export_results(self, output_path: Union[str, Path], 
                      format: str = "xlsx") -> bool:
        """
        åˆ†æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
            format: å‡ºåŠ›å½¢å¼
            
        Returns:
            å‡ºåŠ›æˆåŠŸã®å¯å¦
        """
        if not self.analysis_results:
            raise ValueError("No analysis results. Call analyze() first.")
        
        try:
            output_path = Path(output_path)
            
            if format.lower() == "xlsx":
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    # åŸºæœ¬çµ±è¨ˆ
                    self.analysis_results['basic_stats'].to_excel(
                        writer, sheet_name='Basic_Stats'
                    )
                    
                    # æ¬ æå€¤æƒ…å ±
                    pd.DataFrame(self.analysis_results['missing_values']).to_excel(
                        writer, sheet_name='Missing_Values'
                    )
                    
                    # ç›¸é–¢è¡Œåˆ—
                    if self.analysis_results['correlation_matrix'] is not None:
                        self.analysis_results['correlation_matrix'].to_excel(
                            writer, sheet_name='Correlation'
                        )
            
            elif format.lower() == "json":
                # JSONå½¢å¼ã§ã®å‡ºåŠ›ï¼ˆDataFrameã‚’è¾æ›¸ã«å¤‰æ›ï¼‰
                export_data = {}
                for key, value in self.analysis_results.items():
                    if isinstance(value, pd.DataFrame):
                        export_data[key] = value.to_dict()
                    elif isinstance(value, pd.Series):
                        export_data[key] = value.to_dict()
                    else:
                        export_data[key] = str(value)
                
                import json
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Results exported to {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error exporting results: {e}")
            return False

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    analyzer = DataAnalyzer()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if analyzer.load_data("sample_data.csv"):
        # åˆ†æå®Ÿè¡Œ
        results = analyzer.analyze()
        
        # å¯è¦–åŒ–
        analyzer.visualize("histogram")
        
        # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        analyzer.export_results("analysis_results.xlsx")
```

### ğŸ› ï¸ Copilot Workspaceæ´»ç”¨

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ç”Ÿæˆ
```python
# Copilot Workspace ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
"""
ãƒ‡ãƒ¼ã‚¿åˆ†æWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯:
- Backend: FastAPI + Python
- Frontend: Streamlit
- Database: SQLite
- å¯è¦–åŒ–: Plotly

æ©Ÿèƒ½è¦ä»¶:
1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
3. çµ±è¨ˆåˆ†æãƒ»å¯è¦–åŒ–
4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ãƒ»å±¥æ­´ç®¡ç†

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ :
- app/ (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)
- models/ (ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«)
- utils/ (ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£)
- tests/ (ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰)
- requirements.txt
- README.md
"""

# Copilot ãŒç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
```

---

## Google Geminiå®Ÿè¡Œç’°å¢ƒ

### ğŸ” Gemini ã®ç‰¹å¾´ã¨æ´»ç”¨æ³•

#### ã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–ã®åˆ†æ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆGeminiä½¿ç”¨ï¼‰
"""
ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’é‡è¦–ã—ãŸå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™Pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

è¦ä»¶:
1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†
2. ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
3. é€²æ—è¡¨ç¤º
4. ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½
5. ãƒ­ã‚°å‡ºåŠ›
6. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ

å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: æ•°ç™¾ä¸‡è¡Œã®CSVãƒ•ã‚¡ã‚¤ãƒ«
å‡¦ç†å†…å®¹: çµ±è¨ˆåˆ†æã€é›†è¨ˆã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
åˆ¶ç´„: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€å°é™ã«æŠ‘åˆ¶
"""

# Gemini ç”Ÿæˆã‚³ãƒ¼ãƒ‰ä¾‹
import pandas as pd
import numpy as np
import logging
from pathlib import Path
import yaml
from tqdm import tqdm
import gc

class EfficientDataProcessor:
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’é‡è¦–ã—ãŸå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆæœŸåŒ–"""
        self.config = self.load_config(config_path)
        self.setup_logging()
        
    def load_config(self, config_path: str) -> dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return {
                'chunk_size': 10000,
                'memory_limit_mb': 1000,
                'output_format': 'csv'
            }
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def process_large_file(self, input_path: str, output_path: str):
        """
        å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
        """
        self.logger.info(f"Processing started: {input_path}")
        
        chunk_size = self.config['chunk_size']
        results = []
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = Path(input_path).stat().st_size / (1024 * 1024)  # MB
            self.logger.info(f"File size: {file_size:.1f} MB")
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            chunk_iter = pd.read_csv(input_path, chunksize=chunk_size)
            
            for i, chunk in enumerate(tqdm(chunk_iter, desc="Processing chunks")):
                try:
                    # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®å‡¦ç†
                    processed_chunk = self.process_chunk(chunk)
                    results.append(processed_chunk)
                    
                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    del chunk
                    if i % 10 == 0:  # 10ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«GC
                        gc.collect()
                        
                except Exception as e:
                    self.logger.error(f"Error processing chunk {i}: {e}")
                    continue
            
            # çµæœçµ±åˆ
            final_result = self.combine_results(results)
            
            # å‡ºåŠ›
            self.save_results(final_result, output_path)
            
            self.logger.info("Processing completed successfully")
            
        except Exception as e:
            self.logger.error(f"Processing failed: {e}")
            raise
    
    def process_chunk(self, chunk: pd.DataFrame) -> dict:
        """
        ãƒãƒ£ãƒ³ã‚¯å˜ä½ã®å‡¦ç†
        """
        return {
            'count': len(chunk),
            'sum_values': chunk.select_dtypes(include=[np.number]).sum().to_dict(),
            'mean_values': chunk.select_dtypes(include=[np.number]).mean().to_dict(),
            'null_counts': chunk.isnull().sum().to_dict()
        }
    
    def combine_results(self, results: list) -> dict:
        """
        çµæœã®çµ±åˆ
        """
        combined = {
            'total_count': sum(r['count'] for r in results),
            'sum_values': {},
            'mean_values': {},
            'null_counts': {}
        }
        
        # æ•°å€¤åˆ—ã®åˆè¨ˆå€¤ã‚’çµ±åˆ
        for result in results:
            for col, val in result['sum_values'].items():
                combined['sum_values'][col] = combined['sum_values'].get(col, 0) + val
        
        # å¹³å‡å€¤ã®è¨ˆç®—ï¼ˆåŠ é‡å¹³å‡ï¼‰
        for col in combined['sum_values'].keys():
            combined['mean_values'][col] = combined['sum_values'][col] / combined['total_count']
        
        # NULLå€¤ã‚«ã‚¦ãƒ³ãƒˆã®çµ±åˆ
        for result in results:
            for col, val in result['null_counts'].items():
                combined['null_counts'][col] = combined['null_counts'].get(col, 0) + val
        
        return combined
    
    def save_results(self, results: dict, output_path: str):
        """çµæœä¿å­˜"""
        # çµæœã‚’DataFrameã«å¤‰æ›ã—ã¦ä¿å­˜
        df_results = pd.DataFrame([results])
        df_results.to_csv(output_path, index=False)
        self.logger.info(f"Results saved to: {output_path}")

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹ï¼ˆconfig.yamlï¼‰
config_content = """
chunk_size: 50000
memory_limit_mb: 2000
output_format: csv
processing:
  remove_duplicates: true
  handle_missing: true
  normalize_data: false
logging:
  level: INFO
  file: processing.log
"""

# ä½¿ç”¨ä¾‹
processor = EfficientDataProcessor()
processor.process_large_file("large_dataset.csv", "processed_results.csv")
```

#### Gemini ã®å¤šè¨€èªãƒ»å¤šãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
"""
å¤šè¨€èªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

æ©Ÿèƒ½:
1. è¤‡æ•°è¨€èªã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†
2. ç”»åƒãƒ‡ãƒ¼ã‚¿ã®è§£æ
3. éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
4. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

å¯¾å¿œè¨€èª: æ—¥æœ¬èªã€è‹±èªã€ä¸­å›½èªã€éŸ“å›½èª
å‡ºåŠ›: å¤šè¨€èªå¯¾å¿œãƒ¬ãƒãƒ¼ãƒˆ
"""

class MultiModalAnalyzer:
    """
    å¤šè¨€èªãƒ»å¤šãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        self.supported_languages = ['ja', 'en', 'zh', 'ko']
        self.results = {}
    
    def analyze_text(self, text_data: dict, language: str = 'ja'):
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æ
        """
        import re
        from collections import Counter
        
        if language not in self.supported_languages:
            raise ValueError(f"Unsupported language: {language}")
        
        # è¨€èªåˆ¥ã®å‡¦ç†
        if language == 'ja':
            # æ—¥æœ¬èªå‡¦ç†
            words = re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+', text_data.get('content', ''))
        elif language == 'en':
            # è‹±èªå‡¦ç†
            words = re.findall(r'\b[a-zA-Z]+\b', text_data.get('content', ''))
        
        # é »åº¦åˆ†æ
        word_freq = Counter(words)
        
        return {
            'language': language,
            'word_count': len(words),
            'unique_words': len(word_freq),
            'top_words': word_freq.most_common(10)
        }
    
    def analyze_image(self, image_path: str):
        """
        ç”»åƒãƒ‡ãƒ¼ã‚¿åˆ†æ
        """
        try:
            from PIL import Image
            import numpy as np
            
            img = Image.open(image_path)
            img_array = np.array(img)
            
            return {
                'dimensions': img.size,
                'channels': len(img_array.shape),
                'file_size': Path(image_path).stat().st_size,
                'average_brightness': np.mean(img_array)
            }
        except ImportError:
            return {'error': 'PIL not available'}
    
    def generate_multilingual_report(self, data: dict, target_language: str = 'ja'):
        """
        å¤šè¨€èªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        """
        templates = {
            'ja': {
                'title': 'ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ',
                'summary': 'åˆ†ææ¦‚è¦',
                'details': 'è©³ç´°çµæœ'
            },
            'en': {
                'title': 'Data Analysis Report',
                'summary': 'Analysis Summary',
                'details': 'Detailed Results'
            }
        }
        
        template = templates.get(target_language, templates['ja'])
        
        report = f"""
# {template['title']}

## {template['summary']}
- å‡¦ç†ãƒ‡ãƒ¼ã‚¿æ•°: {data.get('record_count', 0)}
- åˆ†æå¯¾è±¡æœŸé–“: {data.get('period', 'N/A')}
- ä¸»è¦æŒ‡æ¨™: {data.get('key_metrics', {})}

## {template['details']}
{data.get('detailed_results', 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')}
        """
        
        return report

# ä½¿ç”¨ä¾‹
analyzer = MultiModalAnalyzer()

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
text_result = analyzer.analyze_text({
    'content': 'ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚'
}, 'ja')

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
report = analyzer.generate_multilingual_report({
    'record_count': 1000,
    'period': '2024å¹´1æœˆ-6æœˆ',
    'key_metrics': {'å£²ä¸Š': '1å„„å††', 'åˆ©ç›Šç‡': '15%'}
}, 'ja')

print(report)
```

---

## Microsoft Copilotæ´»ç”¨

### ğŸ’¼ Officeçµ±åˆã§ã®æ´»ç”¨

#### Excelé€£æºåˆ†æ
```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ï¼ˆMicrosoft Copilotä½¿ç”¨ï¼‰
"""
Excelãƒ‡ãƒ¼ã‚¿ã‚’Pythonã§åˆ†æã—ã€çµæœã‚’PowerPointãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦è‡ªå‹•ç”Ÿæˆã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

è¦ä»¶:
1. Excel ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
2. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»å‰å‡¦ç†
3. çµ±è¨ˆåˆ†æãƒ»å¯è¦–åŒ–
4. PowerPoint ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ
5. Outlook ã§ã®ãƒ¡ãƒ¼ãƒ«é€ä¿¡

æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯:
- pandas, openpyxl (Excelå‡¦ç†)
- matplotlib, seaborn (å¯è¦–åŒ–)
- python-pptx (PowerPointç”Ÿæˆ)
- win32com.client (Officeé€£æº)
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pptx import Presentation
from pptx.util import Inches
import win32com.client as win32
from datetime import datetime
import os
import tempfile

class OfficeIntegratedAnalyzer:
    """
    Officeçµ±åˆãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        self.data = None
        self.analysis_results = {}
        self.charts = []
    
    def load_excel_data(self, excel_path: str, sheet_name: str = None):
        """
        Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        """
        try:
            if sheet_name:
                self.data = pd.read_excel(excel_path, sheet_name=sheet_name)
            else:
                # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
                excel_file = pd.ExcelFile(excel_path)
                if len(excel_file.sheet_names) == 1:
                    self.data = pd.read_excel(excel_path)
                else:
                    # è¤‡æ•°ã‚·ãƒ¼ãƒˆã®å ´åˆã¯æœ€åˆã®ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨
                    self.data = pd.read_excel(excel_path, sheet_name=0)
            
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape}")
            return True
            
        except Exception as e:
            print(f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def perform_analysis(self):
        """
        ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œ
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # åŸºæœ¬çµ±è¨ˆ
        numeric_cols = self.data.select_dtypes(include=['number']).columns
        
        self.analysis_results = {
            'basic_stats': self.data[numeric_cols].describe(),
            'correlation': self.data[numeric_cols].corr(),
            'missing_values': self.data.isnull().sum(),
            'data_types': self.data.dtypes,
            'record_count': len(self.data)
        }
        
        print("åˆ†æå®Œäº†")
        return self.analysis_results
    
    def create_charts(self):
        """
        ã‚°ãƒ©ãƒ•ä½œæˆ
        """
        numeric_cols = self.data.select_dtypes(include=['number']).columns
        
        # 1. åŸºæœ¬çµ±è¨ˆã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for i, col in enumerate(numeric_cols[:4]):
            if i < len(axes):
                self.data[col].hist(bins=20, ax=axes[i], alpha=0.7)
                axes[i].set_title(f'{col} ã®åˆ†å¸ƒ')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('é »åº¦')
        
        # æœªä½¿ç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
        for i in range(len(numeric_cols), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        chart1_path = os.path.join(tempfile.gettempdir(), 'chart1.png')
        plt.savefig(chart1_path, dpi=300, bbox_inches='tight')
        self.charts.append(chart1_path)
        plt.close()
        
        # 2. ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if len(numeric_cols) > 1:
            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(self.analysis_results['correlation'], 
                       annot=True, cmap='coolwarm', center=0, ax=ax)
            ax.set_title('å¤‰æ•°é–“ç›¸é–¢')
            
            chart2_path = os.path.join(tempfile.gettempdir(), 'chart2.png')
            plt.savefig(chart2_path, dpi=300, bbox_inches='tight')
            self.charts.append(chart2_path)
            plt.close()
        
        print(f"ã‚°ãƒ©ãƒ•ä½œæˆå®Œäº†: {len(self.charts)}å€‹")
    
    def create_powerpoint_report(self, output_path: str):
        """
        PowerPoint ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        """
        # æ–°ã—ã„ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
        prs = Presentation()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰
        title_slide_layout = prs.slide_layouts[0]
        slide = prs.slides.add_slide(title_slide_layout)
        
        title = slide.shapes.title
        subtitle = slide.placeholders[1]
        
        title.text = "ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"
        subtitle.text = f"ä½œæˆæ—¥: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
        
        # ã‚µãƒãƒªãƒ¼ã‚¹ãƒ©ã‚¤ãƒ‰
        bullet_slide_layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(bullet_slide_layout)
        
        title = slide.shapes.title
        content = slide.placeholders[1]
        
        title.text = "åˆ†æã‚µãƒãƒªãƒ¼"
        
        tf = content.text_frame
        tf.text = f"ãƒ‡ãƒ¼ã‚¿æ¦‚è¦"
        
        p = tf.add_paragraph()
        p.text = f"â€¢ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {self.analysis_results['record_count']:,}ä»¶"
        p.level = 1
        
        p = tf.add_paragraph()
        p.text = f"â€¢ æ•°å€¤åˆ—æ•°: {len(self.data.select_dtypes(include=['number']).columns)}åˆ—"
        p.level = 1
        
        p = tf.add_paragraph()
        p.text = f"â€¢ æ¬ æå€¤: {self.analysis_results['missing_values'].sum()}å€‹"
        p.level = 1
        
        # ã‚°ãƒ©ãƒ•ã‚¹ãƒ©ã‚¤ãƒ‰
        for i, chart_path in enumerate(self.charts):
            slide = prs.slides.add_slide(prs.slide_layouts[5])  # blank layout
            
            title_shape = slide.shapes.add_textbox(
                Inches(0.5), Inches(0.5), Inches(9), Inches(1)
            )
            title_frame = title_shape.text_frame
            title_frame.text = f"åˆ†æçµæœ {i+1}"
            
            slide.shapes.add_picture(
                chart_path, Inches(1), Inches(1.5), 
                width=Inches(8), height=Inches(6)
            )
        
        # ä¿å­˜
        prs.save(output_path)
        print(f"PowerPoint ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†: {output_path}")
    
    def send_email_report(self, recipients: list, ppt_path: str):
        """
        Outlook ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ¡ãƒ¼ãƒ«é€ä¿¡
        """
        try:
            # Outlook ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
            outlook = win32.Dispatch('outlook.application')
            
            # ãƒ¡ãƒ¼ãƒ«ä½œæˆ
            mail = outlook.CreateItem(0)  # 0 = olMailItem
            
            # å®›å…ˆè¨­å®š
            mail.To = '; '.join(recipients)
            
            # ãƒ¡ãƒ¼ãƒ«å†…å®¹
            mail.Subject = f"ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ - {datetime.now().strftime('%Y/%m/%d')}"
            
            mail.Body = f"""
ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’é€ä»˜ã„ãŸã—ã¾ã™ã€‚

ã€åˆ†ææ¦‚è¦ã€‘
ãƒ»ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {self.analysis_results['record_count']:,}ä»¶
ãƒ»åˆ†æå®Ÿè¡Œæ—¥: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
ãƒ»æ¬ æå€¤: {self.analysis_results['missing_values'].sum()}å€‹

è©³ç´°ã¯æ·»ä»˜ã®PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚
            """
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜
            mail.Attachments.Add(ppt_path)
            
            # é€ä¿¡
            mail.Send()
            
            print("ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†")
            
        except Exception as e:
            print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    def run_full_analysis(self, excel_path: str, output_dir: str, 
                         recipients: list = None):
        """
        å®Œå…¨ãªåˆ†æãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ
        """
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_excel_data(excel_path):
            return False
        
        # 2. åˆ†æå®Ÿè¡Œ
        self.perform_analysis()
        
        # 3. ã‚°ãƒ©ãƒ•ä½œæˆ
        self.create_charts()
        
        # 4. PowerPoint ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        ppt_path = os.path.join(output_dir, 
                               f"analysis_report_{datetime.now().strftime('%Y%m%d')}.pptx")
        self.create_powerpoint_report(ppt_path)
        
        # 5. ãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if recipients:
            self.send_email_report(recipients, ppt_path)
        
        # 6. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for chart_path in self.charts:
            try:
                os.remove(chart_path)
            except:
                pass
        
        print("å…¨å‡¦ç†å®Œäº†")
        return True

# ä½¿ç”¨ä¾‹
analyzer = OfficeIntegratedAnalyzer()

# å®Œå…¨åˆ†æã®å®Ÿè¡Œ
analyzer.run_full_analysis(
    excel_path="sales_data.xlsx",
    output_dir="./reports",
    recipients=["manager@company.com", "analyst@company.com"]
)
```

---

## LLMçµ„ã¿åˆã‚ã›æˆ¦ç•¥

### ğŸ”„ åŠ¹æœçš„ãªçµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³

#### é–‹ç™ºãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä½¿ã„åˆ†ã‘
```python
class LLMWorkflowManager:
    """
    è¤‡æ•°LLMã‚’åŠ¹æœçš„ã«çµ„ã¿åˆã‚ã›ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†
    """
    
    def __init__(self):
        self.workflow_stages = {
            'planning': 'Claude',      # è©³ç´°ãªè¨­è¨ˆãƒ»è¨ˆç”»
            'coding': 'Copilot',       # é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
            'testing': 'ChatGPT',      # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
            'analysis': 'ChatGPT',     # ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å¯è¦–åŒ–
            'optimization': 'Claude',  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
            'documentation': 'Claude'  # è©³ç´°ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        }
    
    def get_recommended_llm(self, task_type: str, requirements: dict = None):
        """
        ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸæ¨å¥¨LLMé¸æŠ
        """
        cost_priority = requirements.get('cost_priority', False) if requirements else False
        performance_priority = requirements.get('performance_priority', False) if requirements else False
        
        base_recommendation = self.workflow_stages.get(task_type, 'ChatGPT')
        
        # ã‚³ã‚¹ãƒˆé‡è¦–ã®å ´åˆ
        if cost_priority:
            cost_alternatives = {
                'Claude': 'Gemini',
                'ChatGPT': 'Gemini',
                'Copilot': 'Gemini'
            }
            return cost_alternatives.get(base_recommendation, base_recommendation)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ã®å ´åˆ
        if performance_priority and task_type == 'coding':
            return 'Copilot'  # é–‹ç™ºç’°å¢ƒçµ±åˆã§æœ€é«˜æ€§èƒ½
        
        return base_recommendation
    
    def create_hybrid_solution(self, project_requirements: dict):
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¦ä»¶ã«åŸºã¥ããƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ææ¡ˆ
        """
        solution = {
            'primary_llm': None,
            'secondary_llm': None,
            'workflow': [],
            'estimated_cost': 0,
            'expected_performance': 0
        }
        
        # è¦ä»¶åˆ†æ
        is_cost_sensitive = project_requirements.get('budget_constraint', False)
        needs_high_accuracy = project_requirements.get('accuracy_critical', False)
        has_office_integration = project_requirements.get('office_integration', False)
        
        # LLMé¸æŠãƒ­ã‚¸ãƒƒã‚¯
        if has_office_integration:
            solution['primary_llm'] = 'MS Copilot'
            solution['secondary_llm'] = 'ChatGPT'
        elif needs_high_accuracy:
            solution['primary_llm'] = 'Claude'
            solution['secondary_llm'] = 'ChatGPT'
        elif is_cost_sensitive:
            solution['primary_llm'] = 'Gemini'
            solution['secondary_llm'] = 'Copilot'
        else:
            solution['primary_llm'] = 'ChatGPT'
            solution['secondary_llm'] = 'Claude'
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰
        workflow_steps = [
            {'stage': 'requirements_analysis', 'llm': solution['secondary_llm']},
            {'stage': 'system_design', 'llm': solution['secondary_llm']},
            {'stage': 'implementation', 'llm': solution['primary_llm']},
            {'stage': 'testing', 'llm': solution['primary_llm']},
            {'stage': 'optimization', 'llm': solution['secondary_llm']},
            {'stage': 'documentation', 'llm': solution['secondary_llm']}
        ]
        
        solution['workflow'] = workflow_steps
        
        return solution

# ä½¿ç”¨ä¾‹
workflow_manager = LLMWorkflowManager()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¦ä»¶å®šç¾©
project_req = {
    'budget_constraint': True,
    'accuracy_critical': False,
    'office_integration': False,
    'timeline': 'short',
    'team_size': 'small'
}

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ææ¡ˆ
solution = workflow_manager.create_hybrid_solution(project_req)

print("æ¨å¥¨ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³:")
print(f"ä¸»è¦LLM: {solution['primary_llm']}")
print(f"è£œåŠ©LLM: {solution['secondary_llm']}")
print("\nãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:")
for step in solution['workflow']:
    print(f"  {step['stage']}: {step['llm']}")
```

### ğŸ“Š ã‚³ã‚¹ãƒˆãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã®è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
```python
import time
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class LLMUsage:
    llm_name: str
    task_type: str
    tokens_used: int
    cost: float
    execution_time: float
    success: bool
    timestamp: datetime

class LLMCostOptimizer:
    """
    LLMä½¿ç”¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        # æ–™é‡‘è¡¨ï¼ˆæ¦‚ç®—ã€2024å¹´3æœˆæ™‚ç‚¹ï¼‰
        self.pricing = {
            'ChatGPT': {'input': 0.03, 'output': 0.06},  # GPT-4 per 1K tokens
            'Claude': {'input': 0.003, 'output': 0.015},  # Claude-3-Sonnet
            'Copilot': {'monthly': 10.0, 'per_use': 0.0},  # æœˆé¡åˆ¶
            'Gemini': {'input': 0.000125, 'output': 0.000375},  # Gemini Pro
            'MS Copilot': {'monthly': 30.0, 'per_use': 0.0}  # Office 365 Copilot
        }
        
        self.usage_history: List[LLMUsage] = []
        self.monthly_budgets = {}
    
    def estimate_cost(self, llm_name: str, estimated_tokens: int, 
                     task_complexity: str = 'medium') -> float:
        """
        ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š
        """
        if llm_name not in self.pricing:
            return 0.0
        
        pricing = self.pricing[llm_name]
        
        # æœˆé¡åˆ¶ã®å ´åˆ
        if 'monthly' in pricing:
            return 0.0  # æœˆé¡æ–™é‡‘ã«å«ã¾ã‚Œã‚‹
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®å ´åˆ
        complexity_multiplier = {
            'simple': 0.7,
            'medium': 1.0,
            'complex': 1.5
        }
        
        multiplier = complexity_multiplier.get(task_complexity, 1.0)
        adjusted_tokens = estimated_tokens * multiplier
        
        # å…¥åŠ›ãƒ»å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¦‚ç®—æ¯”ç‡ï¼ˆå…¥åŠ›:å‡ºåŠ› = 3:1ï¼‰
        input_tokens = adjusted_tokens * 0.75
        output_tokens = adjusted_tokens * 0.25
        
        total_cost = (
            (input_tokens / 1000) * pricing['input'] +
            (output_tokens / 1000) * pricing['output']
        )
        
        return total_cost
    
    def recommend_llm_for_budget(self, task_requirements: dict, 
                                budget_limit: float) -> str:
        """
        äºˆç®—åˆ¶ç´„ä¸‹ã§ã®æœ€é©LLMæ¨å¥¨
        """
        task_type = task_requirements.get('type', 'general')
        estimated_tokens = task_requirements.get('tokens', 1000)
        quality_requirement = task_requirements.get('quality', 'medium')
        
        # å„LLMã®ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š
        llm_options = []
        
        for llm_name in self.pricing.keys():
            cost = self.estimate_cost(llm_name, estimated_tokens)
            
            if cost <= budget_limit or cost == 0.0:  # æœˆé¡åˆ¶ã¯äºˆç®—å†…ã¨ã¿ãªã™
                # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆä¸»è¦³çš„è©•ä¾¡ï¼‰
                quality_scores = {
                    'ChatGPT': {'coding': 90, 'analysis': 95, 'general': 85},
                    'Claude': {'coding': 93, 'analysis': 88, 'general': 90},
                    'Copilot': {'coding': 95, 'analysis': 70, 'general': 75},
                    'Gemini': {'coding': 72, 'analysis': 80, 'general': 78},
                    'MS Copilot': {'coding': 85, 'analysis': 85, 'general': 80}
                }
                
                quality_score = quality_scores.get(llm_name, {}).get(task_type, 70)
                
                # ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—
                if cost > 0:
                    cost_performance = quality_score / cost
                else:
                    cost_performance = quality_score * 100  # æœˆé¡åˆ¶ã®å ´åˆã¯é«˜ã‚ã«è©•ä¾¡
                
                llm_options.append({
                    'llm': llm_name,
                    'cost': cost,
                    'quality': quality_score,
                    'cost_performance': cost_performance
                })
        
        # ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é †ã«ã‚½ãƒ¼ãƒˆ
        llm_options.sort(key=lambda x: x['cost_performance'], reverse=True)
        
        if llm_options:
            return llm_options[0]['llm']
        else:
            return 'Gemini'  # æœ€å®‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    
    def track_usage(self, llm_name: str, task_type: str, 
                   tokens_used: int, execution_time: float, 
                   success: bool = True):
        """
        ä½¿ç”¨é‡è¿½è·¡
        """
        cost = self.estimate_cost(llm_name, tokens_used)
        
        usage = LLMUsage(
            llm_name=llm_name,
            task_type=task_type,
            tokens_used=tokens_used,
            cost=cost,
            execution_time=execution_time,
            success=success,
            timestamp=datetime.now()
        )
        
        self.usage_history.append(usage)
    
    def generate_cost_report(self, days: int = 30) -> Dict:
        """
        ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_usage = [u for u in self.usage_history if u.timestamp >= cutoff_date]
        
        # LLMåˆ¥é›†è¨ˆ
        llm_summary = {}
        for usage in recent_usage:
            if usage.llm_name not in llm_summary:
                llm_summary[usage.llm_name] = {
                    'total_cost': 0.0,
                    'total_tokens': 0,
                    'success_rate': 0.0,
                    'avg_execution_time': 0.0,
                    'usage_count': 0
                }
            
            summary = llm_summary[usage.llm_name]
            summary['total_cost'] += usage.cost
            summary['total_tokens'] += usage.tokens_used
            summary['avg_execution_time'] += usage.execution_time
            summary['usage_count'] += 1
            
            if usage.success:
                summary['success_rate'] += 1
        
        # å¹³å‡å€¤è¨ˆç®—
        for llm_name, summary in llm_summary.items():
            if summary['usage_count'] > 0:
                summary['success_rate'] = (summary['success_rate'] / summary['usage_count']) * 100
                summary['avg_execution_time'] /= summary['usage_count']
        
        # ç·ã‚³ã‚¹ãƒˆ
        total_cost = sum(s['total_cost'] for s in llm_summary.values())
        
        return {
            'period_days': days,
            'total_cost': total_cost,
            'llm_breakdown': llm_summary,
            'total_requests': len(recent_usage)
        }

# ä½¿ç”¨ä¾‹
optimizer = LLMCostOptimizer()

# ã‚¿ã‚¹ã‚¯è¦ä»¶
task_req = {
    'type': 'coding',
    'tokens': 2000,
    'quality': 'high',
    'deadline': 'urgent'
}

# äºˆç®—åˆ¶ç´„ä¸‹ã§ã®LLMæ¨å¥¨
recommended_llm = optimizer.recommend_llm_for_budget(task_req, budget_limit=0.50)
print(f"æ¨å¥¨LLM: {recommended_llm}")

# ä½¿ç”¨é‡è¿½è·¡ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
optimizer.track_usage('ChatGPT', 'coding', 1500, 45.0, True)
optimizer.track_usage('Claude', 'analysis', 2000, 60.0, True)
optimizer.track_usage('Gemini', 'general', 1000, 30.0, True)

# ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
report = optimizer.generate_cost_report(30)
print(f"\n30æ—¥é–“ã®ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼:")
print(f"ç·ã‚³ã‚¹ãƒˆ: ${report['total_cost']:.2f}")
print(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {report['total_requests']}")

for llm, stats in report['llm_breakdown'].items():
    print(f"\n{llm}:")
    print(f"  ã‚³ã‚¹ãƒˆ: ${stats['total_cost']:.2f}")
    print(f"  æˆåŠŸç‡: {stats['success_rate']:.1f}%")
    print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {stats['avg_execution_time']:.1f}ç§’")
```

---

## ğŸ’¡ å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’1: LLMæ¯”è¼ƒåˆ†æ

```
ã‚¿ã‚¹ã‚¯: åŒä¸€ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ

å†…å®¹:
1. åŒã˜ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¿ã‚¹ã‚¯ã‚’å„LLMã§å®Ÿè¡Œ
2. å®Ÿè¡Œæ™‚é–“ã€ã‚³ãƒ¼ãƒ‰å“è³ªã€è§£èª¬è©³ç´°åº¦ã‚’æ¯”è¼ƒ
3. ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
4. ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ä½œæˆ

è©•ä¾¡æŒ‡æ¨™:
- ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒæˆåŠŸç‡
- è§£èª¬ã®è©³ç´°åº¦
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- å®Ÿè¡Œé€Ÿåº¦
- ã‚³ã‚¹ãƒˆåŠ¹ç‡
```

### æ¼”ç¿’2: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰

```
ã‚¿ã‚¹ã‚¯: è¤‡æ•°LLMã‚’çµ„ã¿åˆã‚ã›ãŸé–‹ç™ºãƒ•ãƒ­ãƒ¼

æ®µéš:
1. è¨­è¨ˆãƒ•ã‚§ãƒ¼ã‚º: Claude
2. å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º: GitHub Copilot
3. ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚º: ChatGPT
4. æœ€é©åŒ–ãƒ•ã‚§ãƒ¼ã‚º: Claude
5. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: Claude

ç›®çš„:
- å„LLMã®å¼·ã¿ã‚’æ´»ã‹ã—ãŸåŠ¹ç‡çš„ãªé–‹ç™º
- ã‚³ã‚¹ãƒˆæœ€é©åŒ–
- å“è³ªå‘ä¸Š
```

### æ¼”ç¿’3: ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

```
ã‚¿ã‚¹ã‚¯: LLMä½¿ç”¨é‡ç›£è¦–ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

æ©Ÿèƒ½:
1. ä½¿ç”¨é‡ãƒ»ã‚³ã‚¹ãƒˆè¿½è·¡
2. äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆ
3. è‡ªå‹•LLMé¸æŠ
4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

æŠ€è¡“è¦ä»¶:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
- äºˆç®—åˆ¶ç´„å¯¾å¿œ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- ã‚³ã‚¹ãƒˆäºˆæ¸¬
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

### LLMãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
- [GitHub Copilot Documentation](https://docs.github.com/en/copilot)
- [Google Gemini API](https://ai.google.dev/)
- [Microsoft Copilot](https://copilot.microsoft.com/)

### æ¯”è¼ƒãƒ»è©•ä¾¡
- [LLM Benchmarks](https://paperswithcode.com/sota/code-generation-on-humaneval)
- [Coding Capabilities Comparison](https://huggingface.co/spaces/bigcode/bigcode-leaderboard)

---

## âœ… ç†è§£åº¦ç¢ºèª

1. å„LLMã®ç‰¹å¾´ã¨é©ç”¨å ´é¢ã‚’ç†è§£ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
2. GitHub Copilotã®çµ±åˆé–‹ç™ºç’°å¢ƒã§ã®æ´»ç”¨ãŒã§ãã¾ã™ã‹ï¼Ÿ
3. Google Geminiã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’æ´»ã‹ã—ãŸæ´»ç”¨ãŒã§ãã¾ã™ã‹ï¼Ÿ
4. Microsoft Copilotã®Officeçµ±åˆæ©Ÿèƒ½ã‚’æ´»ç”¨ã§ãã¾ã™ã‹ï¼Ÿ
5. è¤‡æ•°LLMã‚’çµ„ã¿åˆã‚ã›ãŸæœ€é©ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è¨­è¨ˆã§ãã¾ã™ã‹ï¼Ÿ

---

**æ¬¡ã®è¬›åº§**: [CSVèª­ã¿è¾¼ã¿æ´»ç”¨](14-csv-integration.md)